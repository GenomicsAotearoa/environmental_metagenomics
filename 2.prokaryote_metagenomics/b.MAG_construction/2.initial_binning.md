# Introduction

Similar to assembly, there is no single binning tool that performs best under all conditions but an elegant solution to this problem is present in the tools **DAS_Tool** and **dRep** (discussed later) which take the output of multiple independent binning programs and identify the set of unique bins that best represent the aggregate data.

The process of binning comes down to the use of differences in contig coverage between samples or replicates, and compositional fingerprints between the contigs to cluster assembled reads into 'bins' of contigs that appear to have originated from the same genome. There are a number of tools available for this purpose, and although they all have their own specifics to the implementation, the general approach is similar to that outlined by [Albertsen *et al.* (2013)](https://www.ncbi.nlm.nih.gov/pubmed/23707974). Similar to assembly there is not a single tool that performs best. We therefore bin using a combination of tools and then evaluate the quality of each individual bin.

1. [**MaxBin**](https://sourceforge.net/projects/maxbin/) v2.2.6
1. [**MetaBAT**](https://bitbucket.org/berkeleylab/metabat/src/master/) v2.13
1. [**CONCOCT**](https://concoct.readthedocs.io/en/latest/) v1.1.0

The above softwares are all available as modules in NeSi.

Do note that there are many other excellent binning tools, such as [**GroopM**](http://ecogenomics.github.io/GroopM/) and [**Tetra-ESOM**](https://github.com/tetramerFreqs/Binning). We are also investigating the use of [**VAMB**](https://github.com/RasmussenLab/vamb) as well. At this point in time, we are happy with the outcome of binning using the three tools detailed in this workflow, but there is no disadvantage to adding as many additional tools as you wish.

---

## 1. MetaBAT

The first binning tool we use is **MetaBAT**, since it's quite to run and gives a good first-impression of the expected bin numbers. When running **MetaBAT** it is critical to pay attention to the minimum bin size parameter. By default this is set to 200,000 nucleotides which is fine for most circumstances. However, if you're trying to recover viral genomes it's highly likely that they will fall below this threshold and be exlucded from later analysis. You can lower the minimum bin size using the **-s** flag, although as you lower it you will encounter more low-quality bins. There is no way around this, other than splitting your data into viral/prokaryote subsets prior to binning.

```bash
runMetaBat.sh -t 10 -s 50000 -i contigs.fna -a samples.metabat.txt -o metabat
```

---

## 2. MaxBin

The simplest way to get a coverage table for **MaxBin** is to just recycle the table from **MetaBAT**, and perform a cut on the columns. However, **MaxBin** uses per-sample coverage tables, so you need to make multiple extractions from the **MetaBAT** table. If you have 2 samples, these commands are correct, but for more samples make sure you check the column indices carefully.

```bash
cut -f1,4 samples.metabat.txt > SampleA.maxbin.txt
cut -f1,6 samples.metabat.txt > SampleB.maxbin.txt

run_MaxBin.pl -contig contigs.fna -out maxbin -thread 10 -abund SampleA.maxbin.txt -abund2 SampleB.maxbin.txt
```

---

## 3. CONCOCT

Next is **CONCOCT** published by the Quince lab. **CONCOCT** bins using a significantly different strategy from the other binning tools. While **MetaBAT** and **MaxBin** bin the full contig, the **CONCOCT** workflow cuts contigs into approximately-equal lengthed sub-contigs and creates clusters from these contig fragments. Contigs are then placed into bins according to a majority rules system, based on how many fragments from each contig fall into each cluster.

First, we need to chop up the contigs into 10 kbp fragments, and redistribute the mapping data across the read fragments

```bash
cut_up_fasta.py contigs.fna -c 10000 -o 0 --merge_last -b contigs.10k.bed > contigs.10k.fa

samtools index SampleA.bam
samtools index SampleB.bam

concoct_coverage_table.py contigs.10k.bed SampleA.bam SampleB.bam > contigs.10k.txt
```

Now we are ready to bin:
```bash
# Bin the fragments
concoct --composition_file contigs.10k.fa --coverage_file contigs.10k.txt -b concoct/

# Cluster the fragments back into their original form
merge_cutup_clustering.py concoct/clustering_gt1000.csv > concoct/clustering_merged.csv

# Create the bins
mkdir concoct/fasta_bins/
extract_fasta_bins.py contigs.fna concoct/clustering_merged.csv --output_path concoct/fasta_bins/
```

---

## 5. Bin evaluation

After generating our initial pool of bins from multiple automated binning software, we need to evaluate their quality based on estimated completeness, contamination and strain heterogeneity. Here, we use **CheckM**. **CheckM** is a tool for evaluating the assembly quality of a genome or bin by assessing sets of single-copy markers conserved across either the bacterial or archaeal domain. The actual method used to score the presence of these markers is detailed in the [original publication](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4484387/) but it is quite complex. The gist of the method can be summarised as:

1. Genomes assessed for presence of 120 (bacterial) and 122 (archaeal) single-copy markers
1. Completeness is estimated by the number of markers present and fragmentation of the operons on which they occur
1. Contamination is estimated by the number of duplication observations
1. A rough taxonomy for the genome is inferred using observed markers
1. Completess and comtamination are adjusted for ancestral deletions or duplications

This gives us estimates of the complete and contaminated the each genome or bin. To run **CheckM**:

```bash
checkm lineage_wf \
  -t 20 \
  --pplacer_threads 10 \
  -x fa \
  --tab_table \
  -f bin_summary.txt \
  user_bins/ checkm_output/
```

Take note of the following parameters:

1. **-t**: The number of threads to use for gene prediction
1. **-â€“pplacer_threads** The number of threads to use for phylogenetic placement. Increasing this drastically increases memory consumption
1. **-x** Genome file extension. The default is *.fna* but various binners use *.fasta* or *.fa*. Make sure these are consistent for your final set of bins
1. **--tab_table**: Summarise the results in a tab delimited table
1. **-f**: The name of the file for output


