# Getting started

Once the initial bins are made, they need to be processed down into the best representatives from the data. It is not uncommon for binning programs to produce dozens of bins, but where the majority consisnt of only a few contigs with minimal scientific value. Refinement of bins therefore happens in two steps. First, the bins produced by multiple binning tools are compared to identify which bins are produced by all tools, and which are unique (**DAS_Tool**). The best representative bins are taken from this process, and further dereplicated into species-level representatives across the entire data set (**dRep**). The important difference between these two steps is that **DAS_Tool** works for bins produced from the same assembly, by different binning tools. **dRep** is applied to bins that are distinct at the contig level, but may be closely related in terms of average nucleotide identity.

Once a representative set of preliminary MAGs are produced, we further refine them using emergent self-organising maps to identify robustly bounded MAGs and to tease apart MAGs with overlapping compositional profiles. As a final step, the completeness estimate for each MAG can be determined using **CheckM** so give you an idea of how complete each MAG is. This is done by scanning the genome for the presence of 120 single copy, approximately unlinked, protein markers which are distributed around the average genome. These markers have been chosen based on their near universal presence in bacteria (a different set of 122 is used for archaea), so their loss is typically a sign that a region of the expected genome is missing. **CheckM** does have some knowledge of taxonomic lineages which have lost markers as part of their evolutionary history, and can account for true negative findings for some lineages.

For this workflow, we use the following software:

1. [**DAS_Tool**](https://github.com/cmks/DAS_Tool) v1.1.1
1. [**dRep**](https://github.com/MrOlm/drep) v1.4.3
1. [**CheckM**](https://github.com/Ecogenomics/CheckM) v1.0.12
1. [**VizBin**](http://claczny.github.io/VizBin/)

There are also optional steps for an advanced refinement protocol, which use scripts from David's [github page](https://github.com/dwwaite/bin_detangling).

1. [**compute_kmer_profile.py**](https://github.com/dwwaite/bin_detangling) *in development*
1. [**pseudo_vizbin.py**](https://github.com/dwwaite/bin_detangling) *in development*
1. [**expand_by_mcc.py**](https://github.com/dwwaite/bin_detangling) *in development*

This workflow uses software loaded by the **binning_pipe** alias.

----

### DAS_Tool

**DAS_Tool** is a handy program that looks at the output of multiple binning tools run over a single assembly, and identifies which bins are duplicates of each other. It then dereplicates the initial binning down into a set of non-redundant bins for future processing. This program is simple to use, but requires a bit of preprocessing to get the correct input files. This can generally be done on the command line, but may need to be tweaked to your exact naming and file organisation.

Be aware that if you're asking your binning tool to report unbinned contigs, these commands will pull them into the DAS Tool workflow and mess with results. Make sure to remove/rename any of these files if they are present.

```
# Create a MetaBAT input file
grep ">" metabat/*.fa | sed 's/.fa:>/\t/g' | awk 'OFS="\t" {print $2,$1}' > metabat.txt

# Create a MaxBin input file
grep ">" maxbin/*.fasta | sed 's/.fasta:>/\t/g' | awk 'OFS="\t" {print $2,$1}' > maxbin.txt

# Create a CONCOCT input file
grep ">" concoct/fasta_bins/*.fna | sed 's/.fna:>/\t/g' | awk 'OFS="\t" {print $2,$1}' > concoct.txt

# Run DAS_Tool
mkdir dastool/
DAS_Tool -i metabat.txt,maxbin.txt,concoct.txt -l metabat,maxbin,concoct -t 10 --write_bins 1 \
         --search_engine blast -c contigs.fna -o dastool/
```

----

### dRep

Once we have selected the best 'non-redundant' bins using **DAS_Tool**, we can perform further dereplication via average nucleotide identity (ANI) using **dRep**. Unlike **DAS_Tool**, this tool uses ANI to identify equivalent genomes in a collection, returning only the best-quality representatives. Because this approach is based on genome sequence data rather than contig names, it can be used between assemblies to identify genomes or MAGs that have been detected in multiple samples or assemblies.

**dRep** uses the completeness and contamination estimations from **CheckM** to assess bin quality, and the default behaviour is to run this program over your bins during a run. However, **dRep** does not fully exploit the multithread nature of **CheckM** and this can result in slow run times. I prefer to run **CheckM** first, then reformat the output file into a form that **dRep** can interpret.

*Note: There is a more detailed breakdown of the **CheckM** tool in the [final evaluation](https://github.com/GenomicsAotearoa/environmental_metagenomics/blob/master/metagenomic_binning/6.final_evaluation.md) notebook.*

```
# Run CheckM
checkm lineage_wf -t 20 --pplacer_threads 10 -x fa --tab_table -f bins_for_drep.checkm dastool/_DASTool_bins/ checkm_output/

# Reformat the output
echo "genome,completeness,contamination" > dRep.genomeInfo
cut -f1,12 bins_for_drep.checkm | sed 's/\t/.fa,/g' > p1.txt
cut -f13 bins_for_drep.checkm > p2.txt
paste -d "," p1.txt p2.txt | tail -n+2 >> dRep.genomeInfo

# Run dRep
dRep dereplicate -p 10 --genomeInfo dRep.genomeInfo -g dastool/_DASTool_bins/* dRep_output/
```

----

### Manual bin refinement

Once the initial dereplication has been performed, it is helpful to use a more robust clustering algorithm to view your bins and make sure they they represent distinct clusters of genomic material. For a quick view, you can use the program **VizBin** which provides a great set of tools to perform a visualisation using the *t-Distributed Stochastic Neighbor Embedding* ([t-SNE](https://lvdmaaten.github.io/publications/papers/JMLR_2014.pdf)) algorithm. For a simple approach, you can load your data straight into the **VizBin** program like so:

```
cat dRep_output/dereplicated_genomes/* > all_bins.fna

echo "label" > all_bins.ann.txt

for i in `dRep_output/dereplicated_genomes/*`;
do
    for j in `grep ">" ${i}`;
    do
        echo ${i} >> all_bins.ann.txt
    done
done
```

You can then execute **VizBin** as per the manual, and use the file *all_bins.fna* as the input, and *all_bins.ann.txt* as the annotation file for colouring points. This should give you a good first pass at resolving your bins, but there is one big issue with **VizBin** - it does not allow for multiple coverage values per contig. If you have used differential coverage as part of your binning workflow, and want to make use of this data, we have a set of scripts for performing the *t-SNE* transformation across data with mlutiple coverage values, and allowing the user to up- or down-weight the importance of coverage during the process.

#### Variable-coverage *t-SNE* projection

*Note: This is a part of our workflow that is under active development. Use at your own risk.*

A more advanced way of performing the *t-SNE* projection over a data set is to consider the following factors:

1. Splitting long contigs into smaller pieces, similar to the way **CONCOCT** performs binning. This is helpful for increasing the density of clustering in the *t-SNE*.
1. Using coverage from multiple samples to add dimensionality to the binning process.
1. Varying the relative importance of coverage compared with k-mer weighting.
1. Scaling the k-mer and coverage data prior to performing the *t-SNE* projection.

There are only two scripts we use here, but there are a few parameters that need to be considered for each one.

```
# Chop up the fragments in each bin, then pool them.
# Simulatenously create the annotation file.

echo "label" > all_bins.ann.10k.txt

for i in `dRep_output/dereplicated_genomes/*`;
do
    cut_up_fasta.py ${i} -c 10000 -o 0 --merge_last > ${i}.chop
    for j in `grep ">" ${i}.chop`;
    do
        echo ${i} >> all_bins.ann.10k.txt
    done
    cat ${i}.chop >> all_bins.10k.fa
    rm ${i}.chop
done

# Compute a k-mer distribution file over the 
compute_kmer_profile.py -t 10 -n yeojohnson -k 4 -c contigs.10k.txt all_bins.10k.fa
```

The **compute_kmer_profile.py** calculates the k-mer frequency profile for each contig in the input file, and produces an output table with the form '<input file>.tsv'. There are several parameters you can tune here:

1. **-k**: K-mer size. The default is to use tetranucleotide (k = 4) frequencies, but this can be increased if desired. For difficult to resolve bins, increasing to 5 can improve resolution.
1. **-n**: Normalisation. Peform column-wise (feature-wise) normalisation on the resulting table. The default option is to perform unit variance scaling. Other options are Yeo-Johnson or none.
1. **-c**: Coverage table. Optional, but you can include coverage information as a tab-delimited table where the first column is the contig names, and subsequent columns are coverage values for a particular sample. Sample names will be replaced with names 'Cov1', 'Cov2' etc., and will be scaled the same way as kmers. When adding coverage information, kmer and coverage information is matched using an inner join, so only entries that exist in both the contigs file and coverage table are reported. This means that you can just use the original mapping file, and all contigs that were not included in the final bins will be discarded automatically.
1. **-t**: Number of threads to use.

The actual *t-SNE* projection can be performed using the script **pseudo_vizbin.py**.

```
pseudo_vizbin.py -o all_bins.10k.uniform.tsne all_bins.10k.tsv
pseudo_vizbin.py -w 0.5 -o all_bins.10k.weighted.tsne all_bins.10k.tsv
```

This script takes the output of the **compute_kmer_profile.py** script as a positional argument, and allows you to specify an output name with the parameter **-o**. The important parameter to note in this script is **-w**. This is the weighting factor for the coverage columns.

Weighting of coverage values is an important consideration in performing this transformation, as it is almost a guarantee that you have more observed kmers in your data than coverage sites. For example, if you have an assembly from a pair of sites as in this workflow, then as a percentage of features considered in clustering, then for k=4 your coverage will only account for about 7.7% of the signal in your data.

<img src="https://latex.codecogs.com/svg.latex?\frac{N.\text{&space;}coverage\text{&space;}features}{4!&space;&plus;&space;N.\text{&space;}coverage\text{&space;}features}&space;=&space;\frac{2}{24&space;&plus;&space;2}&space;\approx&space;0.0769" title="\frac{N. coverage features}{4! + N. coverage features} = \frac{2}{24 + 2} \approx 0.0769" title="Column weighting example" />

We can therefore use the **-w** flag to increase the importance of coverage and make abundance a more important feature. To use this parameter, specify a value between 0 and 1 that is the proportion of weighting you want to attribute to coverage. This will then be divided amongst the number of coverage columns, and the remainder divided amongst each of the kmers. I.e.

<img src="https://latex.codecogs.com/svg.latex?Coverage\text{&space;}weighting&space;=&space;\frac{0.5}{N.\text{&space;}coverage\text{&space;}features}&space;=&space;\frac{0.5}{2}&space;=&space;0.25" title="Coverage weighting" />

<img src="https://latex.codecogs.com/svg.latex?Kmer\text{&space;}weighting&space;=&space;\frac{1&space;-&space;0.5}{4!}&space;=&space;\frac{0.5}{24}&space;\approx&space;0.0208" title="Kmer weighting" />

We usually perform clustering using the values in the example below, and then only look at other options if there are problematic regions in the plot.

----