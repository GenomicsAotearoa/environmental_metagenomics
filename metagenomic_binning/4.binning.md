# Getting started

Similar to assembly, there is no single binning tool that performs best under all conditions but an elegant solution to this problem is present in the tools **DAS_Tool** and **dRep** (discussed later) which take the output of multiple independent binning programs and identify the set of unique bins that best represent the aggregate data.

The process of binning comes down to the use of differences in contig coverage between samples or replicates, and compositional fingerprints between the contigs to cluster assembled reads into 'bins' of contigs that appear to have originated from the same genome. There are a number of tools available for this purpose, and although they all have their own specifics to the implementation, the general approach is similar to that outlined by [Albertsen *et al.* (2013)](https://www.ncbi.nlm.nih.gov/pubmed/23707974). Similar to assembly there is not a single tool that performs best. We therefore bin using a combination of tools and then evaluate the quality of each individual bin.

1. [**MaxBin**](https://sourceforge.net/projects/maxbin/) v2.2.4
1. [**MetaBAT**](https://bitbucket.org/berkeleylab/metabat/src/master/) v2.12.1
1. [**CONCOCT**](https://concoct.readthedocs.io/en/latest/) v0.4.1

Do note that there are many other excellent binning tools, such as [**GroopM**](http://ecogenomics.github.io/GroopM/) and [**Tetra-ESOM**](https://github.com/tetramerFreqs/Binning). We are also investigating the use of [**VAMB**](https://github.com/RasmussenLab/vamb) as well. At this point in time, we are happy with the outcome of binning using the three tools detailed in this workflow, but there is no disadvantage to adding as many additional tools as you wish.

This workflow uses software loaded by the **genome_pipe** alias.

----

### Coverage tables

One issue that can slow down projects is that that read mapping is required for every binning tool, and the default behaviour for each program is to map the reads against the assembly. This is OK for a single run, but when multiple bining tools are intended for use, it is a waste of resources to map the same reads against the same assembly three different times. We will take the result of the single mapping above and parse the results into inputs for each mappping tool below.

#### Compress sam files into bam format

We can use **samtools** to generate *bam* files from the initial *sam* alignment.

```
samtools view -uS SampleA.sam | samtools sort -O bam -o SampleA.bam
samtools view -uS SampleB.sam | samtools sort -O bam -o SampleB.bam
```

----

### MetaBAT

The first binning tool we use is **MetaBAT**, since it's quite to run and gives a good first-impression of the expected bin numbers. When running **MetaBAT** it is critical to pay attention to the minimum bin size parameter. By default this is set to 200,000 nucleotides which is fine for most circumstances. However, if you're trying to recover viral genomes it's highly likely that they will fall below this threshold and be exlucded from later analysis. You can lower the minimum bin size using the **-s** flag, although as you lower it you will encounter more low-quality bins. There is no way around this, other than splitting your data into viral/prokaryote subsets prior to binning.

```
jgi_summarize_bam_contig_depths --outputDepth Samples.metabat.txt SampleA.bam SampleB.bam

runMetaBat.sh -t 10 -s 50000 -i contigs.fna -a metabat.txt -o metabat
```

----

### MaxBin

The simplest way to get a coverage table for **MaxBin** is to just recycle the table from **MetaBAT**, and perform a cut on the columns. However, **MaxBin** uses per-sample coverage tables, so you need to make multiple extractions from the **MetaBAT** table. If you have 2 samples, these commands are correct, but for more samples make sure you check the column indices carefully.

```
cut -f1,4 SampleA.metabat.txt > SampleA.maxbin.txt
cut -f1,6 Samples.metabat.txt > SampleB.maxbin.txt

run_MaxBin.pl -contig contigs.fna -out maxbin -thread 10 -abund SampleA.maxbin.txt -abund2 SampleB.maxbin.txt
```

----

### CONCOCT

Next is **CONCOCT** published by the Quince lab. **CONCOCT** bins using a significantly different strategy from the other binning tools. While **MetaBAT** and **MaxBin** bin the full contig, the **CONCOCT** workflow cuts contigs into approximately-equal lengthed sub-contigs and creates clusters from these contig fragments. Contigs are then placed into bins according to a majority rules system, based on how many fragments from each contig fall into each cluster.

In order to run **CONCOCT** on NeSI, you first need to activate the a **conda** environment.

```
source activate /nesi/project/uoa02469/Software/PipelineSetup/concoct_env
```

First, we need to chop up the contigs into 10 kbp fragments, and redistribute the mapping data across the read fragments

```
cut_up_fasta.py contigs.fna -c 10000 -o 0 --merge_last -b contigs.10k.bed > contigs.10k.fa

samtools index SampleA.bam
samtools index SampleB.bam
concoct_coverage_table.py contigs.10k.bed SampleA.bam SampleB.bam > contigs.10k.txt
```

Now we are ready to bin:

```
# Bin the fragments
concoct --composition_file contigs.10k.fa --coverage_file contigs.10k.txt -b concoct/

# Cluster the fragments back into their original form
merge_cutup_clustering.py concoct/clustering_gt1000.csv > concoct/clustering_merged.csv

# Create the bins
mkdir concoct/fasta_bins/
extract_fasta_bins.py contigs.fna concoct/clustering_merged.csv --output_path concoct/fasta_bins/
```

----