# Introduction

While the identification of phage has rapidly increased in the metagenomics age, matching identified phage to their range of host organisms remains a significant limitation of the field. For phage identified in cultured prokaryote isolates or known to infect specific eukaryotic cells, the pairing of phage to host is immediately known (although, the full range of possible hosts may still require further study). In contrast, in the jumbled sea of assembled contigs in a metagenomics study, identifying specific pairings between host (such as prokaryote) genomes and individual viral genomes is a much more complex task.

Understanding the host, or range of hosts, that a virus is capable of infecting is vital to understanding viral effects on individual organisms in an ecosystem as well as the dynamics of the ecosystem as a whole. There is also revitalised interest in the use of phage therapy to target "problematic" microbes (such as "pathogenic" bacteria associated with disease) as an alternative to antibiotics. And while phage discovery is in rapid growth, accurately understanding the range of organisms that many of these phage can infect, and their potential direct effect on those organisms, is currently lagging behind.

Several tools and methods have been developed that attempt to address this limitation in viral metagenomics, with the aim of predicting matches between viral genomes and the likely target host that they infect. Some of these include:

- Machine learning-based approaches [**RaFAH**](https://sourceforge.net/projects/rafah/) [(Coutinho et al., 2021)](https://doi.org/10.1016/j.patter.2021.100274), [**HostG**](https://github.com/KennthShang/HostG) [(Shang & Sun, 2021)](https://doi.org/10.1186/s12915-021-01180-4), and [**VirHostMatcher**](https://github.com/jessieren/VirHostMatcher) [(Ahlgren et al., 2017)](https://doi.org/10.1093/nar/gkw1002)
- tRNA pairwise **BLAST** matching between identified viral tRNA and prokaryote metagenome-assembled genome (MAG) tRNA sequences
- Nucleotide sequence identity based on pairwise **BLAST** between viral genomes and full prokaryote MAGs
- **BLAST** matching of CRISPR spacers identified in viral genomes against prokaryote MAGs
- Identifying integrated prophage genomes contained within larger contigs (including stretches of host genome) that were also binned into specific prokaryote MAGs

In practice, several of these approaches (such as the machine learning-based tools) are limited by the fact that reference databases on which they're developed currently still include a small fraction of the likely full viral diversity. We have found that several of the other approaches (such as tRNA BLAST matching and nucleotide sequence identity) can provide an indication of likely match, but are not definitive. CRISPR matching is currently the most robust approach, but in our experience only returns a limited number of hits. Finally, prophage identified directly in binned MAGs have the caveat that differing GC content and coverage (particularly if the virus is replicating at the time) of the viral region within the contig may have affected the bin placement, and this contig may not actually correctly belong to the prokaryote MAG that it has been assigned to. Furthermore, each of the latter approaches (CRISPR; tRNA; genome sequence identity) require having available relevant prokaryote references, such as a set of MAGs from the same system (although, in practice these can also be applied to external reference databases as well).

<!--ref required-->
In previous studies a hierarchy of the reliability of each tool has been applied. And ultimately, concordance of multiple methods is ideal. However, in our experience with complex metagenomics data sets, results from each of these approaches are often contradictory, making interpretation difficult, and much work remains to be done in progressing this field.

With these caveats in mind, examples are provided below for several of these approaches (CRISPR spacer matching, viral contigs binned in MAGs, tRNA pairwise **BLAST**, genome nucleotide identity), together with some helper scripts to generate easily readable summary tables. 

> **Limitations of the methods below**<br>
> The extent to which the diversity contained within the MAGs data set reflects the full diversity of the system is limited. Often, MAGs are a simplified set of the reality of the system. As such, we might expect to miss a lot of matches due to gaps in the available data. Furthermore, some amount of micro-diversity is likely to be lost in the process of dereplicating MAGs (e.g. **dRep** applied to MAGs from multiple assemblies). To minimise the loss of micro-diversity, it may be preferable to apply this step to use the set of MAGs prior to this dereplication. For example, having refined and reduced MAG sets for each assembly (e.g. via **DASTool** dereplication of MAGs generated by multiple tools for each assembly, followed by manual curation), but prior to a final dereplication across all assemblies via **dRep**. This is more likely to maximise the micro-diversity in the MAG set and may result in an increased number of hits for spacer and/or tRNA matches. The micro-diversity in MAG sequences could include some of the sites associated with CRISPR spaces, tRNA sequences, and even whole viral genomes. This will also retain useful information on the specific assemblies/samples that particular virus-host CRISPR and/or tRNA matches were identified in, and whether they were identified across multiple sites (which may strengthen confidence in the prediction).

# CRISPR spacer matching

This method involves:

1. Using [**Crass**](https://ctskennerton.github.io/crass/) [(Skennerton et al. 2013)](https://doi.org/10.1093/nar/gkt183) to help identify and extract CRISPR spacer sequences in the data set of filtered and trimmed sequencing reads (prior to assembly)
1. Pairwise BLAST searches are then run between:
   - spacer sequences vs viral genomes
   - spacer sequences vs a set of prokaryote metagenome-assembled genomes.
1. These results can then be compiled to identify spacer sequences that match between specific viral genomes and specific MAGs.


### Data prep

Concatenate prokaryote MAGs and copy over read files.

> **WARNING**<br>
> It might be tempting to use the original reads themselves to save space. However, if **Crass** crashes, you may need to [excise problematic reads](#troubleshooting). By copying the reads and using the secondary copy, you can safely make changes while retaining the integrity of the original reads for other analyses. ***NEVER ALTER ORIGINAL SEQUENCE FILES***.

```bash
# Concatenate bins into all.hosts.fna
mkdir -p host_prediction/crispr/bins
cat /path/to/bin_files/*.fa > host_prediction/crispr/bins/all.hosts.fna

# Copy over reads files
mkdir -p host_prediction/crispr/infiles
cp /path/to/wgs/Qual_filtered_trimmomatic/*.fastq host_prediction/crispr/infiles/
```

### Extract crispr spacers

1. Identify CRISPR spacer sequences from filtered sequencing reads via **Crass**
1. Get stats via `crisprtools stat` (part of **Crass** install)
1. Extract spacer sequences
1. Add sample info into headers of spacer sequences

> **Note to NeSI users**<br>
> **Crass** is currently unavailable as a NeSI module. Please install your own copy following these [instructions](https://ctskennerton.github.io/crass/assets/manual.pdf#section.2).

```bash
# Navigate to working directory
cd host_prediction

# Define paths
crass_path=/nesi/project/uoa02469/Software/crass/bin

# Output directories
mkdir -p crispr/sampleA         # CRISPR assembly
mkdir -p crispr/stats_out       # Stats
mkdir -p crispr/spacer_seqs     # Spacer sequences

# Run Crass
${crass_path}/crass \
  crispr/infiles/sampleA_R1.fastq \
  crispr/infiles/sampleA_R2.fastq \
  -o crispr/sampleA/

# Get stats via crisprtools stats
${crass_path}/crisprtools stat \
  -ap crispr/sampleA/crass.crispr \
  > crispr/stats_out/sampleA_crass_stats.txt

# Extract spacer sequences
${crass_path}/crisprtools extract \
  -o crispr/ \
  -O crispr/sampleA/ \
  -s crispr/sampleA/crass.crispr \
  > crispr/spacer_seqs/sampleA_spacers.fa

# Add sample info to sequence headers of spacers.fa files
sed -i "s/>/>sampleA_/g" crispr/spacer_seqs/sampleA_spacers.fa
```

#### Recommended SLURM variables

| Variables | Values  |
| :-------- | :------ |
| Time      | 6 hours |
| Memory    | 3GB     |
| CPU       | 2       |

#### Troubleshooting

If Crass crashes, it may be necessary to filter out problematic reads that cause **Crass** to crash (the output displays FastQ headers for problematic sequences) then re-run it on the filtered read files. 

> **WARNING**
> Modify the ***COPIED*** read files.

Example where multiple reads from samples S8 and S9 failed:

- sed here identifies the header row based on ID and deletes this row and the 3 that follow (since sequences cover four lines in fastq file format).
- Note that the read IDs for R1 and R2 will be identical except for the 1 or 2 at the start of the section after the space)

Here, we will use `sed` to filter the read file. We provide it the sequence identifier (output of **Crass** error) as pattern then delete it and the following 3 lines. We make sure to delete the sequence and its pair (`R1` and `R2`).  

> **Reminder on the FastQ format**<br>
> FastQ files contain these 4 lines/objects per record/sequence
> 1. Sequence identifier (starts with '@')
> 1. Sequence letters
> 1. Sequence identifier (starts with '+')
> 1. Quality scores


```bash
# Navigate to working directory
cd host_prediction/crispr/infiles

# Remove problematic reads
sed -i '/^@7001326F:173:H27CLBCX3:1:2114:12778:70365 1:N:0:ATTCAGAA+GTACTGAC/,+3d' S8_R1.fastq
sed -i '/^@7001326F:173:H27CLBCX3:1:2114:12778:70365 2:N:0:ATTCAGAA+GTACTGAC/,+3d' S8_R2.fastq
```

> **Note for NeSI users**<br>
> You may need to run this as a **SLURM** job for large file sizes as `sed` requires the entire file to be read into memory while processing

### BLAST analysis: spacers vs viral contigs and MAGs

Run **BLAST** comparison between spacers and vOTUs and MAGs (bins)

> **Note**<br>
> The associated helper script to generate summary tables requires that `-outfmt` be set exactly as below.

```bash
# Navigate to working directory
cd host_prediction/

# Load modules
module purge
module load BLAST/2.9.0-gimkl-2018b

# Output directories
mkdir -p crispr/spacers_blastn

# Concatenate spacers together
cat crispr/spacer_seqs/*.fa > crispr/spacer_seqs/all_spacer_seqs.fna

# Build index
makeblastdb \
  -in crispr/spacer_seqs/all_spacer_seqs.fna \
  -dbtype nucl \
  -out crispr/spacer_seqs/all_spacer_seqs.fna

# Define output format
OUTFMT='6 qseqid qlen sseqid slen pident length mismatch gapopen qstart qend sstart send evalue bitscore qcovs'

# BLASTn against MAGs
blastn \
  -num_threads 6 \
  -query crispr/bins/all.hosts.fna \
  -db crispr/spacer_seqs/all_spacer_seqs.fna \
  -outfmt ${OUTFMT} \
  -out crispr/spacers_blastn/blastn_crisprSpacers.Bins.txt

# BLASTn against vOTUs
blastn \
  -num_threads 6 \
  -query ../checkv_vOTUs/vOTUs.checkv_filtered.fna \
  -db crispr/spacer_seqs/all_spacer_seqs.fna \
  -outfmt ${OUTFMT} \
  -out crispr/spacers_blastn/blastn_crisprSpacers.vOTUs.txt
```

### Compile CRISPR spacers summary results table

The code below includes filtering to keep only **BLAST** matches with at most 1 mismatch over the full length of the spacer sequence. It also adds predicted GTDB taxonomy for each of the matching MAGs (if this is unavailable, it is useful to run **GTDB-Tk** on your MAGs first, or modify the script below to remove GTDB-related steps) The variable `n_hits_threshold` at the beginning sets the number of top matches to keep for each **BLAST** search.

> **Note**<br>
> This will ultimately be put into a script for ease of use. But for now we can use the python code below

```bash
# Navigate to working directory
cd host_prediction

# Output directories
mkdir -p summary_tables

# Load modules
module purge
module load Python/3.8.2-gimkl-2020a

# Call python
python3
```
```py
import pandas as pd
import numpy as np
import re
import glob

# Number of top hits to keep
n_hits_threshold = 3

# File paths
gtdb_taxonomy_path = '/path/to/gtdb/output'
crispr_results_bins = 'crispr/spacers_blastn/blastn_crisprSpacers.Bins.txt'
crispr_results_votus = 'crispr/spacers_blastn/blastn_crisprSpacers.vOTUs.txt'

# MAGs
## Read in GTDB taxonomy to append to each
gtdb_df = pd.concat([pd.read_csv(f, sep='\t') for f in glob.glob("/path/to/gtdb/output/*.summary.tsv")],
                      ignore_index=True)[['user_genome', 'classification']]
gtdb_df.columns = ['crispr_blast_binID', 'crispr_blast_bin_taxonomy_gtdb']

## Read in BLAST results
bins_df = pd.read_csv(crispr_results_bins, sep='\t', header=None)

## Rename columns
bins_df.columns = ['bin_contig_ID', 'query_length', 'spacer_ID', 'spacer_len', 'pident', 'match_length', 'mismatch', 'gapopen', 'query_start', 'query_end', 'spacer_start', 'spacer_end', 'evalue', 'bitscore', 'query_covs']

## Filter to only keep matches with at most 1 mismatch over the full length of the spacer sequence
bins_df = bins_df[(bins_df['spacer_len'] == bins_df['match_length']) & (bins_df['mismatch'] <= 1)]

# vOTUs
## Read in BLAST results
df = pd.read_csv(crispr_results_votus, sep='\t', header=None)

## Rename columns
df.columns = ['virID', 'query_length', 'spacer_ID', 'spacer_len', 'pident', 'match_length', 'mismatch', 'gapopen', 'query_start', 'query_end', 'spacer_start', 'spacer_end', 'evalue', 'bitscore', 'query_covs']

## Filter to only keep matches with ≤ 1 mismatch over the full length of the spacer sequence
df = df[(df['spacer_len'] == df['match_length']) & (df['mismatch'] <= 1)]

# Join with bins results (by spacer_ID) and filter to keep only rows that have hits for both a viral contig and a binned contig
df = pd.merge(df, bins_df, how="left", on="spacer_ID", suffixes=("_vir", "_bin"))
df = df[df['bin_contig_ID'].notnull()].sort_values(by=['virID'])

## Add GTDB taxonomy for bins
df['crispr_blast_binID'] = df['bin_contig_ID'].str.replace(r'_NODE.*', '')
df = pd.merge(df, gtdb_df, how="left", on="crispr_blast_binID").reset_index(drop=True)

## Filter to keep columns of interest
df = df[['virID', 'pident_vir', 'evalue_vir', 'bitscore_vir', 'pident_bin', 'evalue_bin', 'bitscore_bin', 'crispr_blast_binID', 'crispr_blast_bin_taxonomy_gtdb']]
df.columns = ['virID', 'crispr_blast_pident_vir', 'crispr_blast_evalue_vir', 'crispr_blast_bitscore_vir', 'crispr_blast_pident_bin', 'crispr_blast_evalue_bin', 'crispr_blast_bitscore_bin', 'crispr_blast_binID', 'crispr_blast_bin_taxonomy_gtdb']

# Filter to only keep top n hits for each vOTU_ID
## ERROR handling: If n_hits_threshold greater than or equal to max counts, need to modify n_hits_threshold
MAX_VALUE_COUNTS = df.groupby('virID')['virID'].value_counts().max()
if n_hits_threshold >= MAX_VALUE_COUNTS:
    n_hits_threshold_edit = MAX_VALUE_COUNTS-1
else:
    n_hits_threshold_edit = n_hits_threshold

## Filter by n_hits_threshold
df = df[df.index.isin(df.groupby('virID')['crispr_blast_evalue_vir'].nsmallest(n_hits_threshold_edit).index.get_level_values(1))].sort_values(by=['virID', 'crispr_blast_evalue_vir'])

## Pivot wider and apply suffix to multiple hits
df['idx'] = '_'+(df.groupby(['virID']).cumcount() + 1).astype(str)
df = (
    df.pivot_table(
        index=['virID'], 
        columns=['idx'], 
        values=['crispr_blast_pident_vir', 'crispr_blast_evalue_vir', 'crispr_blast_bitscore_vir', 'crispr_blast_pident_bin', 'crispr_blast_evalue_bin', 'crispr_blast_bitscore_bin', 'crispr_blast_binID', 'crispr_blast_bin_taxonomy_gtdb'], 
        aggfunc='first'
    )
)
df.columns = [''.join(col) for col in df.columns]
df = df.reset_index()

# Write out summary table
df.to_csv('summary_tables/summary_table.crisprSpacers_blastn.tsv', sep='\t', index=False)

quit()
```

<br>

# Binned viral contigs

This method aims to identify any viral contigs that were co-binned when generating the prokaryote MAGs data set. Note that this requires
- `genome2contig_lookupTable.tsv` matching contig IDs of all contigs binned into MAGs against their MAG (bin) ID (two columns, with headers `contigID` and `binID`). See [here](3.virus_coverage_profile.md#create-genome2contig_lookuptabletsv) for more information and some python code to generate this table.
- `vOTUs_lookupTable.txt` was originally generated during dereplication of viral contigs into vOTUs (see [here](1.identify_viral_sequences.md#optional-modify-derep-contig-headers-to-be-votu_n)).

Optionally, the script below also incorporates MAG taxonomy (**GTDB** results) and **CheckM**, as these are useful metrics to include here.

> **Note**<br>
> This will ultimately be put into a script for ease of use. But for now we can use the python code below.

```bash
# Output directories
mkdir -p host_prediction/binned_contigs
mkdir -p host_prediction/summary_tables

# Load modules
module purge
module load Python/3.8.2-gimkl-2020a

# Call python
python3
```
```py
import pandas as pd
import numpy as np
import re
from Bio.SeqIO.FastaIO import SimpleFastaParser
import os

# File paths
votus_lookupTable_path = 'viral_identification/cluster_vOTUs/vOTUs_lookupTable.txt'
viral_contigs_path = 'viral_identification/checkv_vOTUs/vOTUs.checkv_filtered.fna'
MAGs_bin2contig_lookupTable_path = '/path/to/MAGs_bin2contig_lookupTable.tsv'
MAGs_checkM_summary_path = '/path/to/checkm_bin_summary.txt'
MAGS_gtdb_output_path = '/path/to/gtdb/output'

# vOTUs
votus_dict = {}
with open(viral_contigs_path, 'r') as read_fasta:
    for name, seq in SimpleFastaParser(read_fasta):
        votus_dict[name] = seq

# Convert to dataframe
votu_df = pd.DataFrame.from_dict(votus_dict.items())
votu_df.columns = ['vOTU_ID_full', 'seq']
votu_df['vOTU_ID'] = votu_df['vOTU_ID_full'].str.replace(r'(vOTU_\d+).*', r'\1', regex=True)

# Join with lookuptable to get original contigIDs
lookupTable_df = pd.read_csv(votus_lookupTable_path, sep='\t')

# Trim contigID back to orignal form
lookupTable_df['contigID'] = lookupTable_df['cluster_rep_contigID'].str.replace(r'(cov_.*\.\d+).*', r'\1', regex=True)
votu_df = pd.merge(votu_df, lookupTable_df, left_on="vOTU_ID", right_on="vOTU", how="left")

# Join with MAGs bin2contig lookupTable to identify any matches between the datasets
mag_bin2contig_df = pd.read_csv(MAGs_bin2contig_lookupTable_path, sep='\t')
votu_df = pd.merge(votu_df, mag_bin2contig_df, on="contigID", how="left")

# Binned vOTU contigs (exclude any rows with no matches between vOTUs and MAGs)
binned_votus_df = votu_df[~votu_df['binID'].isna()]

# For reference: Counts of binIDs with vOTU matches (Open to view)
vir2bin_matches_df = binned_votus_df['binID'].value_counts().rename_axis('bins').reset_index(name='counts')

# Write out results table
binned_votus_df = binned_votus_df[['vOTU_ID_full', 'binID']]
binned_votus_df.columns = ['virID', 'cobinned_binID']
binned_votus_df.to_csv('host_prediction/binned_contigs/vOTUs.cobinned.tsv', sep='\t', index=False)

# Optional: read in MAG taxonomy (gtdb) and checkM and join with cobinned results
gtdb_df = pd.concat([pd.read_csv(f, sep='\t') for f in glob.glob(MAGS_gtdb_output_path+"/*.summary.tsv")],
                      ignore_index=True)[['user_genome', 'classification']]
gtdb_df.columns = ['binID', 'cobinned_bin_taxonomy_gtdb']
checkm_df = pd.read_csv(MAGs_checkM_summary_path, sep='\t')[['Bin Id', 'Completeness', 'Contamination', 'Strain heterogeneity']]
checkm_df.columns = ['binID', 'cobinned_checkm_Completeness', 'cobinned_checkm_Contamination', 'cobinned_checkm_Strain heterogeneity']
mag_stats_df = pd.merge(gtdb_df, checkm_df, how="outer", on="binID").reset_index(drop=True).rename(columns={"binID": "cobinned_binID"})
# Join MAG stats with cobinned results
full_df = pd.merge(binned_votus_df, mag_stats_df, how="left", left_on="cobinned_binID", right_on="cobinned_binID").reset_index(drop=True)

# Write out summary table
full_df.to_csv('host_prediction/summary_tables/summary_table.vir_contigs.cobinned.tsv', sep='\t', index=False)

quit()
```

<br>

# tRNA pairwise BLAST

Here we compare tRNA sequences (predicted using [ARAGORN](http://www.ansikte.se/ARAGORN/) [(Laslett & Canback, 2004)](https://doi.org/10.1093%2Fnar%2Fgkh152)) between MAGs and viral contigs using BLAST. As with CRISPR matching, using cross-assembly dereplicated MAGs might lead to a loss of some micro-diversity. If that is a concern, use MAGs prior to this dereplication. This is more likely to maximise the micro-diversity in the MAG set (and potentailly the diversity of tRNA) and may result in an increased number of hits for tRNA matches. This will also retain useful information on the specific assemblies (samples, in this case) that particular tRNA matches were identified in, and whether they were identified across multiple sites (which may strengthen confidence in the prediction).

> Note for NeSI users
> ARAGORN is not currently available as a NeSI module. You will need to download and install this first (see [here](http://www.ansikte.se/ARAGORN/Downloads/)).

### Set up working directory and copy infiles over

NOTE:

- In the next step, we will predict tRNA sequences for each individual "genome" via aragorn. Here we will first generate individual files for each "genome" (contig) in the vOTUs data set. A simple python script to achieve this is given below.
- If your bin (MAG) file extensions are something other than .fa (e.g. .fna), ammend the script below in both the bin files copy step and the step writing out the individual viral contig files (so that all extensions match for ease of use downstream), and also in the subsequent aragorn step below.

```bash
# Output directories
mkdir -p host_prediction/tRNA_blast/infiles

# bin files 
cp /path/to/bin_files/*.fa host_prediction/tRNA_blast/infiles/

# Load module
module purge
module load Python/3.8.2-gimkl-2020a

# Call python
python3
```
```py
# # Split virus fasta file into individual sequences
import pandas as pd
import numpy as np
from Bio.SeqIO.FastaIO import SimpleFastaParser

with open('viral_identification/checkv_vOTUs/vOTUs.checkv_filtered.fna', 'r') as read_fasta:
    for name, seq in SimpleFastaParser(read_fasta):
        with open('host_prediction/tRNA_blast/infiles/'+name+'.fa', 'w') as write_fasta:
            write_fasta.write(">" + str(name) + "\n" + str(seq) + "\n")

quit()
```

### Identify tRNA

Before running **ARAGORN** on each "genome" to identify tRNAs for pairwise BLAST, we need to:
1. Remove any empty files (i.e. those without predicted tRNAs)
2. Modify contig headers in ARAGORN output (simplifies downstream use)
3. Concatenate tRNA predictions (one file each from viral contigs and MAGs)

> **Note on header renaming**<br>
> The concatenation step for tRNA files relating to MAGs is currently written based on naming generated by metabat, maxbin, and concoct. If this does not match your MAG file names, ammend this line in the code below (e.g. if your MAG files are all simply labelled "MAG_n.fna" then cat MAG_*.trna.fna > concatenated_tRNA/host_tRNA.fna will suffice)

```bash
# Navigate to working directory
cd host_prediction/tRNA_blast

# Output directory
mkdir -p aragorn_out

# Run ARAGORN
for file in infiles/*.fa; do
  filename=$(asename ${file} .fa)
  /path/to/Software/aragorn_v1.2.38/aragorn \
    -t -gcstd -l -a -q -rn -fon \
    -o aragorn_out/${filename}.aragorn \
    ${file}
done

# Navigate to output directory
cd aragorn_out

# Remove empty files
find . -type f -empty -delete

# Modify headers
for file in *aragorn; do 
    filename=$(basename ${file} .aragorn)
    sed -e "s/>/>${filename}_/g" -e 's/\(.*\) .*/\1/' -e 's/[^a-zA-Z0-9>]/_/g' -e 's/_$//g' ${file} > ${file}.trna.fna
done

# Concatenate tRNA predictions
mkdir -p concatenated_tRNA

cat vOTU*.trna.fna > concatenated_tRNA/viral_tRNA.fna
cat *metabat*.trna.fna *maxbin*.trna.fna *concoct*.trna.fna > concatenated_tRNA/host_tRNA.fna

# Compile tRNA with aragorn headers in full 
# (i.e. the original aragorn output files)
mkdir -p concatenated_tRNA_aragorn

cat vOTU*.aragorn > concatenated_tRNA_aragorn/viral_tRNA.fna
cat *metabat*.aragorn *maxbin*.aragorn *concoct*.aragorn > concatenated_tRNA_aragorn/host_tRNA.fna
```

#### Recommended SLURM variables

| Variables | Value   |
| :-------- | :------ |
| Time      | 3 hours |
| Memory    | 1GB     |
| CPUs      | 12      |

### Pairwise BLAST of tRNA sequences

> **Note**<br>
> `-outfmt` must be exactly as below for the subsequent script that generates the summary table.

```bash
# Load modules
module purge
module load BLAST/2.9.0-gimkl-2018b

# Navigate to working directory
cd host_prediction/tRNA_blast

# Create BLAST database from concatenated MAG tRNA
makeblastdb \
  -dbtype nucl \
  -in aragorn_out/concatenated_tRNA/host_tRNA.fna \
  -out aragorn_out/concatenated_tRNA/host_tRNA.fna

# Pairwise BLAST of viral_tRNA v. host_tRNA
blastn \
  -query aragorn_out/concatenated_tRNA/viral_tRNA.fna \
  -db aragorn_out/concatenated_tRNA/host_tRNA.fna \
  -num_threads 8 \
  -outfmt "6 qseqid qlen sseqid slen pident length mismatch gapopen qstart qend sstart send evalue bitscore qcovs" \
  -num_alignments 5 \
  -perc_identity 90 -dust no \
  -out vOTUs.tRNA_blast_virus_host.txt
```

### Compile tRNA BLAST filtered summary results table

This code below is similar to the one [above](#compile-crispr-spacers-summary-results-table). The main difference is that it only retains hits that have at least 90% nucleotide identity over at least 90% of the length of the shorter sequence.

```bash
# Navigate to working directory
cd /working/dir/host_prediction

# Output directories
mkdir -p summary_tables

# Load modules
module purge
module load Python/3.8.2-gimkl-2020a

# Call python
python3
```
```py
import pandas as pd
import numpy as np
import re
import glob

# Set threshold
n_hits_threshold = 3

# File paths
gtdb_taxonomy_path = '/path/to/gtdb/output'
trna_blast_path = 'tRNA_blast/vOTUs.tRNA_blast_virus_host.txt'

# Read in gtdb taxonomy to append to each
gtdb_df = pd.concat([pd.read_csv(f, sep='\t') for f in glob.glob(gtdb_taxonomy_path+"/*.summary.tsv")],
                      ignore_index=True)[['user_genome', 'classification']]
gtdb_df.columns = ['tRNA_blast_binID', 'tRNA_blast_bin_taxonomy_gtdb']

# Read in and process BLAST results
df = pd.read_csv(trna_blast_path', sep='\t', header=None)

# Rename columns
df.columns = ['vir_tRNA_ID', 'vir_tRNA_length', 'bin_tRNA_ID', 'bin_tRNA_len', 'pident', 'match_length', 'mismatch', 'gapopen', 'vir_tRNA_start', 'vir_tRNA_end', 'bin_tRNA_start', 'bin_tRNA_end', 'evalue', 'bitscore', 'vir_covs']

# Add virID column
df['virID'] = df['vir_tRNA_ID'].str.replace(r'(vOTU_.*)_.*_.*_tRNA.*', r'\1')

# Add % length of sequence match
df['match_length_pct'] = ((df['match_length'] / df[["vir_tRNA_length", "bin_tRNA_len"]].min(axis=1))*100).round(2)

# Filter to only keep matches with ≥ 90% nucleotide identity and ≥ 90% length of (shortest) tRNA sequence
df = df[(df['pident'] >= 90) & (df['match_length_pct'] >= 90)]

## Add gtdb taxonomy for bins
df['tRNA_blast_binID'] = df['bin_tRNA_ID'].str.replace(r'(contigs).*', r'\1').str.replace(r'_', r'.').str.replace(r'.sub', r'_sub')
df = pd.merge(df, gtdb_df, how="left", on="tRNA_blast_binID").reset_index(drop=True)

## Filter to keep columns of interest
df = df[['virID', 'tRNA_blast_binID', 'tRNA_blast_bin_taxonomy_gtdb', 'pident', 'evalue', 'bitscore']]
df.columns = ['virID', 'tRNA_blast_binID', 'tRNA_blast_bin_taxonomy_gtdb', 'tRNA_blast_pident', 'tRNA_blast_evalue', 'tRNA_blast_bitscore']

# Filter to only keep top n hits for each vOTU_ID
## ERROR handling: If n_hits_threshold greater than or equal to max counts, need to modify n_hits_threshold
MAX_VALUE_COUNTS = df.groupby('virID')['virID'].value_counts().max()
if n_hits_threshold >= MAX_VALUE_COUNTS:
    n_hits_threshold_edit = MAX_VALUE_COUNTS-1
else:
    n_hits_threshold_edit = n_hits_threshold

## Filter based on n_hits_threshold
df = df[df.index.isin(df.groupby('virID')['tRNA_blast_evalue'].nsmallest(n_hits_threshold_edit).index.get_level_values(1))].sort_values(by=['virID', 'tRNA_blast_evalue'])
# pivot wider and apply suffix to multiple hits
df['idx'] = '_'+(df.groupby(['virID']).cumcount() + 1).astype(str)
df = (df.pivot_table(index=['virID'], 
                               columns=['idx'], 
                               values=['tRNA_blast_binID', 'tRNA_blast_bin_taxonomy_gtdb', 'tRNA_blast_pident', 'tRNA_blast_evalue', 'tRNA_blast_bitscore'], 
                               aggfunc='first'))
df.columns = [''.join(col) for col in df.columns]
df = df.reset_index()

# Write out summary table
df.to_csv('summary_tables/summary_table.tRNA_blast_virus_host.tsv', sep='\t', index=False)

quit()
```

# Whole genome nucleotide sequence identity

Pairwise BLAST searches between vOTUs and MAGs to assess whole genome sequence identity

> **Note**<br>
> `-outfmt` must be exactly as below for the subsequent script that generates the summary table.

```bash
# Navigate to working directory
cd host_prediction

# Output directories
mkdir -p pairwise_blast/infiles
cat /path/to/bin_files/*.fa > pairwise_blast/infiles/all.hosts.fna

# Load modules
module purge
module load BLAST/2.9.0-gimkl-2018b

# Generate database from all.hosts.fna
makeblastdb \
  -dbtype nucl \
  -in pairwise_blast/infiles/all.hosts.fna  \
  -out pairwise_blast/infiles/all.hosts.fna

# Pairwise BLAST against vOTUs (BLAST pairwise comparisons based on nucleotide sequence identity)
blastn \
  -num_threads 10 \
  -query viral_identification/checkv_vOTUs/vOTUs.checkv_filtered.fna \
  -db pairwise_blast/infiles/all.hosts.fna \
  -outfmt "6 qseqid qlen sseqid slen pident length mismatch gapopen qstart qend sstart send evalue bitscore qcovs" \
  -perc_identity 70 \
  -out pairwise_blast/vOTUs.pairwise_blast.txt
```

### Compile filtered summary results tables

This script includes a filtering step to keep only BLAST matches with: 
- E-value at most 0.001
- Nucleotide identity at least 70% 
- Bit-score ≥ 50
  
> **Note**<br>
> This will ultimately be put into a script for ease of use. But for now we can use the python code below.

```bash
# Navigate to working directory
cd host_prediction

# Output directory
mkdir -p summary_tables

# Load modules
module purge
module load Python/3.8.2-gimkl-2020a

# Call python
python3
```
```py
import pandas as pd
import numpy as np
import re
import glob

# Set number of top hits to keep
n_hits_threshold = 3

# File paths
gtdb_taxonomy_path = '/path/to/gtdb/output'
nucleotide_identity_blast_path = 'pairwise_blast/vOTUs.pairwise_blast.txt'

# Read in gtdb taxonomy to append to each
gtdb_df = pd.concat([pd.read_csv(f, sep='\t') for f in glob.glob(gtdb_taxonomy_path+"/*.summary.tsv")],
                      ignore_index=True)[['user_genome', 'classification']]
gtdb_df.columns = ['pairwise_blast_binID', 'pairwise_blast_bin_taxonomy_gtdb']

# Read in and process BLAST results
df = pd.read_csv(nucleotide_identity_blast_path, sep='\t', header=None)

## Rename columns
df.columns = ['virID', 'vir_length', 'binID', 'bin_len', 'pident', 'match_length', 'mismatch', 'gapopen', 'vir_start', 'vir_end', 'bin_start', 'bin_end', 'evalue', 'bitscore', 'vir_covs']

## Filter to only keep matches with: e-value ≤ 0.001 & nucleotide identity ≥ 70% & bit-score ≥ 50 
df = df[(df['pident'] >= 70) & (df['evalue'] <= 0.001) & (df['bitscore'] >= 50)]

## Add gtdb taxonomy for bins
df['pairwise_blast_binID'] = df['binID'].str.replace(r'(contigs).*', r'\1').str.replace(r'_', r'.').str.replace(r'.sub', r'_sub')
df = pd.merge(df, gtdb_df, how="left", on="pairwise_blast_binID").reset_index(drop=True)

## Filter to keep columns of interest
df = df[['virID', 'pairwise_blast_binID', 'pairwise_blast_bin_taxonomy_gtdb', 'pident', 'evalue', 'bitscore']]
df.columns = ['virID', 'pairwise_blast_binID', 'pairwise_blast_bin_taxonomy_gtdb', 'pairwise_blast_pident', 'pairwise_blast_evalue', 'pairwise_blast_bitscore']

# Filter to only keep top n hits for each vOTU_ID
## ERROR handling: If n_hits_threshold greater than or equal to max counts, need to modify n_hits_threshold
MAX_VALUE_COUNTS = df.groupby('virID')['virID'].value_counts().max()
if n_hits_threshold >= MAX_VALUE_COUNTS:
    n_hits_threshold_edit = MAX_VALUE_COUNTS-1
else:
    n_hits_threshold_edit = n_hits_threshold

## Filter based on n_hits_threshold
df = df[df.index.isin(df.groupby('virID')['pairwise_blast_evalue'].nsmallest(n_hits_threshold_edit).index.get_level_values(1))].sort_values(by=['virID', 'pairwise_blast_evalue'])

## Pivot wider and apply suffix to multiple hits
df['idx'] = '_'+(df.groupby(['virID']).cumcount() + 1).astype(str)
df = (df.pivot_table(index=['virID'], 
                               columns=['idx'], 
                               values=['pairwise_blast_binID', 'pairwise_blast_bin_taxonomy_gtdb', 'pairwise_blast_pident', 'pairwise_blast_evalue', 'pairwise_blast_bitscore'], 
                               aggfunc='first'))
df.columns = [''.join(col) for col in df.columns]
df = df.reset_index()

# Write out summary table
df.to_csv('summary_tables/summary_table.pairwise_blast.tsv', sep='\t', index=False)

quit()
```

<br>

# Host prediction full summary table

Generate a summary table combining all host prediction results.

- Output files to compile:
  - crispr spacers: host_prediction/summary_tables/summary_table.crisprSpacers_blastn.tsv
  - binned viral contigs: host_prediction/summary_tables/summary_table.vir_contigs.cobinned.tsv
  - tRNA BLAST: host_prediction/summary_tables/summary_table.tRNA_blast_virus_host.tsv
  - pairwise BLAST: host_prediction/summary_tables/summary_table.pairwise_blast.tsv
- The script below also adds vOTU checkV stats, as this can be useful when assessing predicted host matches

> **Note**<br>
> This will ultimately be put into a script for ease of use. But for now we can use the python code below.

```bash
# Load modules
module purge
module load Python/3.8.2-gimkl-2020a

# Call python
python3
```
```py
import pandas as pd
import numpy as np
import re

# Import all results tables
## CRISPR BLAST
crispr_df = pd.read_csv('host_prediction/summary_tables/summary_table.crisprSpacers_blastn.tsv', sep='\t')

## Binned viral contigs
cobinned_df = pd.read_csv('host_prediction/summary_tables/summary_table.vir_contigs.cobinned.tsv', sep='\t')

## tRNA BLAST
tRNA_df = pd.read_csv('host_prediction/summary_tables/summary_table.tRNA_blast_virus_host.tsv', sep='\t')

## Pairwise BLAST
pairwise_df = pd.read_csv('host_prediction/summary_tables/summary_table.pairwise_blast.tsv', sep='\t')

## CheckV results
checkv_df = pd.read_csv('1.viral_identification/6.checkv_vOTUs/vOTUs.checkv_filtered_quality_summary.tsv', sep='\t', ignore_index=True).drop(columns=['Unnamed: 0', 'provirus', 'proviral_length']).add_prefix('checkv_')

# Join all results tables together into summary table
summary_df = pd.merge(
    tRNA_df, how="outer", on=['virID', 'dataset']).merge(
    pairwise_df, how="outer", on=['virID', 'dataset']).merge(
    crispr_df, how="outer", on=['virID', 'dataset']).merge(
    cobinned_df, how="outer", on=['virID', 'dataset']).sort_values(by=['dataset', 'virID']).reset_index(drop=True)

## Trim contigID to match checkv's output 
## (trim off the '__checkv_excised...' bits that we added earlier)
summary_df['contig_id'] = summary_df['virID'].str.replace(r'_\d+__checkv_excised.*', r'', regex=True)

## Merge CheckV results onto the summary table
summary_df = pd.merge(summary_df, checkv_df, how="left", left_on='contig_id', right_on='checkv_contig_id').drop(columns=['contig_id'])

# Reorder columns
summary_df=summary_df[['dataset', 'virID'] + 
                       [col for col in summary_df.columns if 'checkv' in col] + 
                       [col for col in summary_df.columns if 'cobinned' in col] + 
                       [col for col in summary_df.columns if 'crispr' in col] + 
                       [col for col in summary_df.columns if 'tRNA' in col] + 
                       [col for col in summary_df.columns if 'pairwise' in col]]

# Write out summary table
summary_df.to_csv('host_prediction/summary_tables/summary_table.All_host_predictions.tsv', sep='\t', index=False)

quit()
```
