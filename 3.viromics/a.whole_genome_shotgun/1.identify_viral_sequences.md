# Introduction

Viral metagenomics is a rapidly progressing field, and new software are constantly being developed and released each year that aim to better identify and characterise viral genomic sequences from assembled metagenomic sequence reads. Currently, the most commonly used methods are **VirSorter2**, **VIBRANT**, and **VirFinder** (or the machine learning implementation of this, **DeepVirFinder**). Each tool has strengths and weaknesses. And given this is an evolving field, none are perfect. A number of recent studies use either one of these tools, or a combination of several at once.

Here, we use a combination of three tools:

- [**VirSorter2**](https://github.com/jiarong/VirSorter2) ([Guo et al., 2015](https://doi.org/10.7717/peerj.985), [Guo et al., 2021](https://doi.org/10.1186/s40168-020-00990-y)) uses a predicted protein homology reference database-based approach, together with searching for a number of pre-defined metrics based on known viral genomic features. **VirSorter2** has been designed to target dsDNA phages, ssDNA and RNA viruses, and the viral groups *Nucleocytoviricota* and *Lavidaviridae*.

- [**VIBRANT**](https://github.com/AnantharamanLab/VIBRANT) ([Kieft et al., 2020](https://doi.org/10.1186/s40168-020-00867-0)) uses a machine learning approach based on protein similarity (non-reference-based similarity searches with multiple HMM sets), and is in principle applicable to bacterial and archaeal DNA and RNA viruses, integrated proviruses (which are excised from contigs by VIBRANT), and eukaryotic viruses.

- [**DeepVirFinder**](https://github.com/jessieren/DeepVirFinder) ([Ren et al., 2017](https://doi.org/10.1186/s40168-017-0283-5), [Ren et al., 2020](https://doi.org/10.1007/s40484-019-0187-4)) uses a machine learning based approach based on k-mer frequencies. Having developed a database of the differences in k-mer frequencies between prokaryote and viral genomes, **VirFinder** examines assembled contigs and identifies whether their k-mer frequencies are comparable to known viruses in the database, using this to predict viral genomic sequence. It has some limitations based on the viruses that were included when building the database (bacterial DNA viruses, but very few archaeal viruses, and, at least in some versions of the software, no eukaryotic viruses). However, tools are also provided to build your own database should you wish to develop an expanded one. Due to its distinctive k-mer frequency-based approach, **VirFinder** may also have the capability of identifying some novel viruses overlooked by tools such as **VIBRANT** or **VirSorter**. However, it will also likely have many more false-positives, and so requires more careful curation. It also appears to no longer be in development or support, so may be outdated compared to, for example, **VirSorter2**.

> **Expedited workflow**<br>
> At the time of writing, if you were to pick one tool, we would recommend **VirSorter2**, as this appears to have the most recent ongoing development work and also ties in nicely with **DRAM-v** (for [viral annotation](2.viral_gene_annotation.md)).

> **Sequence dereplication**<br>
> Similar to the prokaryotic metagenomics pipeline for binning, within-assembly (thus, cross-tool) dereplication must occur before cross-assembly dereplication. In other words, if you use more than one software/tool for viral sequence identification *AND* have multiple assemblies, you must dereplicate sequences within each assembly across different software outputs *THEN* dereplicate sequences across assemblies. Within-assembly dereplication is performed using `summarise_viral_contigs.py` and `virome_per_sample_derep.py`, while cross-assembly dereplication is performed using `Cluster_genomes_5.1.pl`.

<br>

# Prepare environment

The following workflow requires several custom scripts found here. It is advised to add the path to the directory where these scripts are in.

```bash
# Prepend path to scripts to $PATH
export PATH="/path/to/scripts:$PATH"
```

When running the following as a **SLURM** job, remember to append the above to the top of the **SBATCH** script like so:

```bash
#!/bin/bash -e
#SBATCH --account <project>
#SBATCH --time    <time> 
# <other SLURM header and variables>

# Prepend path to scripts
export PATH="/path/to/scripts:$PATH"
```

# Prepare assembly files

<!-- DUPLICATED
### Optional: Add assembly ID to contig headers

If you now have data from multiple assemblies, it can be useful to add the assembly ID to contig headers to avoid conflicts downstream (on very rare occasions, contigs from different assemblies can end up with identical contig headers), and to make it easier to spot where contigs of interest originated from after dereplication.

```bash
cd /working/dir
mkdir -p /path/to/wgs/assembly/2.spades_assembly_edit

# Individual sample assemblies
for i in {1..9}; do
    sed "s/>/>S${i}_/g" /path/to/wgs/assembly/1.spades_assembly_S${i}/scaffolds.fasta > /path/to/wgs/assembly/2.spades_assembly_edit/S${i}.assembly.fasta
done 
```
-->

### Filter out short contigs

For downstream processing, it can be a good idea to filter out short contigs (for example, those less than 1000 or 2000 bp). VIBRANT, for example, recommends removing contigs < 1000 bp, as it then filters based on presence of 4 identified putative genes, rather than contig length.

If you wish to filter out short contigs, you can do so via seqmagick:

```bash
# Load seqmagick
module purge
module load seqmagick/0.7.0-gimkl-2017a-Python-3.6.3

## Filter out contigs < 1000 bp
seqmagick convert --min-length 1000 assembly.fasta assembly.m1000.fasta
```

<br>

# VirSorter2

In the steps below, we first run VirSorter2 with a min score threshold setting of 0.75. A python script is provided to then filter these results to only retain contigs with a score > 0.9 or if they have a viral hallmark gene identified. There are no set rules on how best to set filter thresholds here, but the latter roughly follow the screening thresholds discussed in the [VirSorter2 protocols page](https://www.protocols.io/view/viral-sequence-identification-sop-with-virsorter2-5qpvoyqebg4o/v3).

> **Note to NeSI users**<br>
> 1. Unload the XALT module (using `module unload XALT`) before loading VirSorter. This is due to an installation issue.
> 1. Reference databases are not part of the NeSI module and must be downloaded separately ([instructions below](#download-reference-databases)).
> 1. We recommend including the argument `--config LOCAL_SCRATCH=${TMPDIR:-/tmp}` due to how the software handles temporary files.

> **Note on viral groups**<br>
> For version 2.2.3, the argument `--include_groups` must be provided with all available groups listed. There is ongoing work to include an 'all' option in later releases (see [changelog](https://github.com/jiarong/VirSorter2/blob/master/Changelog.md#unreleased)).

### Download reference databases
If you do not already have the databases available, you can download these using `virsorter setup`.

```bash
# Set up database directory 
# (you may want to name the database directory with the date downloaded)
mkdir -p virsorter2_database

# Load module
module purge
module unload XALT
module load VirSorter/2.2.3-gimkl-2020a-Python-3.8.2

# Download databases
virsorter setup -d virsorter2_database
```

### Run VirSorter2

```bash
# Load modules
module purge
module unload XALT
module load VirSorter/2.2.3-gimkl-2020a-Python-3.8.2

# Output directories
mkdir -p viral_identification/virsorter2
 
## Run virsorter2
srun virsorter run \
  -j 32 \
  -i assembly.m1000.fasta \
  -d virsorter2_database/ \
  --min-score 0.75 \
  --include-groups dsDNAphage,NCLDV,RNA,ssDNA,lavidaviridae \
  -w viral_identification/virsorter2/sampleA \
  -l sampleA \
  --rm-tmpdir \
  all \
  --config LOCAL_SCRATCH=${TMPDIR:-/tmp}
```

#### Recommended SLURM variables
| Variable | Value    |
| :------- | :------- |
| Time     | 12 hours |
| Memory   | 20GB     |
| CPUs     | 32       |


#### Filter outputs
Retain contigs with a score $\ge$ 0.9 *or* if they have a viral hallmark gene identified

```bash
# Load module
module purge
module load Python/3.8.2-gimkl-2020a
python3
```

```py
import pandas as pd
import numpy as np

## Filter results (score >= 0.9 OR hallmark > 0)
# Loop through all samples, and output new 'SampleX-final-viral-score_filt_0.9.tsv' file for each.
for number in range(1, 10):
    # Load ...final-viral-score.tsv file
    vsort_score = pd.read_csv('1.viral_identification/1.virsorter2/S'+str(number)+'/S'+str(number)+'-final-viral-score.tsv', sep='\t')
    # Filter by score threshold and/or hallmark gene (e.g. score >= 0.9 OR hallmark > 0)
    vsort_score = vsort_score[np.logical_or.reduce((vsort_score['max_score'] >= 0.9, vsort_score['hallmark'] > 0))]
    # Write out filtered file
    vsort_score.to_csv('1.viral_identification/1.virsorter2/S'+str(number)+'/S'+str(number)+'-final-viral-score_filt_0.9.tsv', sep='\t', index=False)

quit()
```

<!--
Alternatively, in `awk`

```bash
awk -F"\t" '$7 >= 0.9 || $10 > 0 {print $0}' sampleA-final-viral-score.tsv > filtered-sampleA-final-viral-score.tsv
```
-->

<br>

# VIBRANT

Run VIBRANT

```bash
# Load modules
module purge
module load VIBRANT/1.2.1-gimkl-2020a

# Output directories
mkdir -p viral_identification/vibrant

# Run main analyses 
srun VIBRANT_run.py \
  -t 16 \
  -i assembly.m1000.fasta \
  -d $DB_PATH \
  -folder viral_identification/vibrant
```

#### Recommended SLURM variables

| Variable | Value    |
| :------- | :------- |
| Time     | 12 hours |
| Memory   | 20GB     |
| CPUs     | 16       |

> **Note**<br>`$DB_PATH` is pre-defined with the loading of **VIBRANT**.

# DeepVirFinder

In addition to running the software, the authors also recommends calculating the false discovery rate (FDR) of the results. This is performed in R aided by the **qvalue** package implemented in the `dvfind_add_fdr.R` script. Be aware that the software runs the risk of identifying eukaryotic sequences as putative viral sequences due to the k-mer profiling model training being done on bacterial (and maybe archaeal) vs viral sequences in databases. Therefore, it is important to identify (using Kraken2) and remove eukaryotic sequences from the output.

> **Note to NeSI users**<br>
> Unlike **VirSorter2** and **VIBRANT**, DeepVirFinder is not installed as a NeSI module and must be installed manually (see [instructions below](#installing-deepvirfinder)).

### Install DeepVirFinder

The following DeepVirFinder workflow requires the following dependencies:
- **theano** (Python)
- **keras** (Python)
- **qvalue** (R)

```bash
# Clone git and add permissions
git clone https://github.com/jessieren/DeepVirFinder
chmod -R 777 DeepVirFinder

# Load python and R
module purge
module load Python/3.8.2-gimkl-2020a R/3.6.2-gimkl-2020a

# Install Python dependencies
pip install theano keras

# Install R dependencies
R --vanilla
if (!requireNamespace("BiocManager", quietly = TRUE))
    install.packages("BiocManager")
BiocManager::install("qvalue")
q()
```

### Run DeepVirFinder

```bash
# Add paths
export PATH="~/.local/bin:$PATH" # Allows use of installed Python dependencies
DeepVirFinder_PATH="/path/to/Software/DeepVirFinder:$PATH"

# Load modules
module purge
module load Python/3.8.2-gimkl-2020a
module load R/3.6.2-gimkl-2020a

# Define variables
export MKL_THREADING_LAYER=GNU

# Output directories
mkdir -p viral_identification/deepvirfinder

# Run analyses
${DeepVirFinder_PATH}/dvf.py \
  -i assembly.m1000.fasta \
  -m ${DeepVirFinder_PATH}/models \
  -o viral_identification/deepvirfinder \
  -l 1000 \
  -c 20

# Calculate false discovery rate
dvfind_add_fdr.R "viral_identification/deepvirfinder/assembly.m1000.fasta_gt1000bp_dvfpred.txt"
```

### Remove eukaryotic sequences

1. Extract sequences for contigs identified as putatively 'viral' by DeepVirFinder (`dvfpred_extract_fasta.py`)
1. Assign taxonomy via **Kraken2**
1. Extract sequences matching Eukaryota (`extract_kraken_reads.py`, from [**KrakenTools**](https://github.com/jenniferlu717/KrakenTools))
1. Filter Eukaryota sequences out of DeepVirFinder results (`dvfpred_filter_euk.py`)

```bash
# Add paths
export PATH="~/.local/bin:$PATH"

# Load modules
module purge
module load Python/3.8.2-gimkl-2020a R/3.6.2-gimkl-2020a Kraken2/2.1.2-GCC-9.2.0

# Extract putative viral sequences
dvfpred_extract_fasta.py \
  --deepvirfinder_results viral_identification/deepvirfinder/assembly.m1000.fasta_gt1000bp_dvfpred.txt \
  --assembly_fasta assembly.m1000.fasta \
  --output viral_identification/deepvirfinder/dvfpred.fna

# Run Kraken2
mkdir -p viral_identification/deepvirfinder/dvfpred_kraken

kraken2 \
  --threads 20 \
  --db nt \
  --use-names \
  --report viral_identification/deepvirfinder/dvfpred_kraken/kraken_report.txt \
  --output viral_identification/deepvirfinder/dvfpred_kraken/kraken_output.txt \
  viral_identification/deepvirfinder/dvfpred.fna

# Extract eukaryotic sequences
extract_kraken_reads.py \
  -k viral_identification/deepvirfinder/dvfpred_kraken/kraken_output.txt \
  -r viral_identification/deepvirfinder/dvfpred_kraken/kraken_report.txt \
  -s viral_identification/deepvirfinder/dvfpred.fna \
  -t 2759 \
  --include-children \
  -o viral_identification/deepvirfinder/dvfpred_kraken/kraken_Euk.fna

# Filter DeepVirFinder result
dvfpred_filter_euk.py \
  --deepvirfinder_results viral_identification/deepvirfinder/assembly.m1000.fasta_gt1000bp_dvfpred.txt \
  --Euk_fasta viral_identification/deepvirfinder/dvfpred_kraken/kraken_Euk.fna \
  --output viral_identification/deepvirfinder/dvfpred_filtered.txt
```

> **Note**<br> 
> The argument `-t 2759` for `extract_kraken_reads.py` represents Eukaryota

#### Recommended SLURM variables

| Variables | Values     |
| :-------- | :--------- |
| Time      | 10 minutes |
| Memory    | 180GB      |
| CPUs      | 20         |

#### Troubleshooting

If DeepVirFinder was unable to detect "viral" sequences in the assembly, the filtering steps will likely fail. In this case, copy the empty file `assembly.m1000.fasta_gt1000bp_dvfpred.txt` to `dvfpred_filtered.txt` to resume the following steps.

<br>

# Summary tables per assembly

The custom script `summarise_viral_contigs.py` generates a summary table from all (or any combination of) the tools run above. Not all inputs are required (e.g. if you excluded **DeepVirFinder**, this can be omitted here)

```bash
# Load modules
module purge
module load Python/3.9.9-gimkl-2020a

# Output directories
mkdir -p summary_tables

# Run script
summarise_viral_contigs.py \
  --vibrant viral_identification/vibrant/VIBRANT_results_assembly.m1000/VIBRANT_summary_results_assembly.m1000.tsv \
  --virsorter2 viral_identification/virsorter2/sampleA-final-viral-score_filt_0.9.tsv \
  --deepvirfinder viral_identification/deepvirfinder/dvfpred_filtered.txt \
  --out_prefix summary_tables/viral_contigs.summary_table
```

<br>

# Dereplication (within-assembly)

If multiple viral sequence identification tools were used, it is necessary to dereplicate the identified sequences to generate a set of unique viral contigs for an assembly. The goal is to create a single file of combined results from all employed methods using information from the summary table.

Some items to keep in mind:
- Both **VirSorter2** and **VIBRANT** can return prophage sequences that have been excised out of the original assembly contigs which may also contain contaminating host sequence on either end of the prophage sequence.
- With how `virome_per_sample_derep.py` is currently written, all prophage genomic regions excised from contigs (i.e. after trimming off host regions) by either **VIBRANT** or **VirSorter2** are all retained (**VIBRANT** IDs for excised prophage have 'fragment' in the contig header, **VirSorter2** IDs include 'partial'). If both tools excise a prophage from the same contig, both will be retained here (i.e. there will likely be a number of duplicates of prophage sequences retained). But this is ok, as they will be dereplicated downstream in the `Cluster_genomes_5.1.pl` step.
- In cases where any tool (**VirSorter2** or **VIBRANT**) has excised a prophage from a contig, this script also ensures that the original full contig is not retained, even if it as been identified as 'viral' by one of the other tools (e.g. DeepVirFinder). This is to ensure that the excised prophage is not simply re-integrated into the full contig (with contaminating host sequence on either end) and lost during the downstream **Cluster_genomes_5.1.pl** dereplication step.

```bash
# Output directories
mkdir -p viral_identification/perSample_derep

# Load modules
module purge
module load Python/3.9.9-gimkl-2020a

# Run script
virome_per_sample_derep.py
  --assembly_fasta assembly.m1000.fasta \
  --summary_table summary_tables/viral_contigs.summary_table \
  --vibrant viral_identification/vibrant/VIBRANT_phages_assembly.m1000/assembly.m1000.phages_combined.fna \
  --virsorter2 viral_identification/virsorter2/final-viral-combined.fa \
  --output viral_identification/3.perSample_derep/viral_contigs.fna
```

### Filter (within-assembly) dereplicated contigs

Confidence in the viral calls of each of the tools generally increases with contig length. As such, various studies have only retained contigs greater than a set threshold (e.g. 3,000 bp, 5,000 bp, or 10,000 bp). This also can assist with reducing the dataset to a manageable size in large and complex metagenome datasets.

The code below filters to retain contigs with length $\ge$ 5000 bp

```bash
# Load modules
module purge
module load seqmagick/0.7.0-gimkl-2017a-Python-3.6.3

seqmagick convert --min-length 5000 \
  viral_identification/perSample_derep/viral_contigs.fna \
  viral_identification/perSample_derep/viral_contigs.filt.fna
```

## Quality assessment and additional filtering

### CheckV
[**CheckV**](https://bitbucket.org/berkeleylab/checkv/src/master/) ([Nayfach et al., 2021](https://doi.org/10.1038/s41587-020-00774-7)) is a tool that has been developed as an analogue to **CheckM**. **CheckV** provides various statistics about the putative viral contigs data set, including length, gene count, viral and host gene counts, and estimated completeness and contamination. We can run **CheckV** on the output from dereplication of contigs identified by the three methods, and use the results from **CheckV** as an additional filtering step prior to our final dereplication.

```bash
# Load modules
module purge
module load CheckV/0.7.0-gimkl-2020a-Python-3.8.2

# Output directories
mkdir -p viral_identification/perSample_checkv

# Define variables
checkv_in="viral_identification/perSample_derep/viral_contigs.filt.fna"
checkv_out="viral_identification/perSample_checkv/"

# Run CheckV
checkv end_to_end ${checkv_in} ${checkv_out} -t 16 --quiet
```

#### Recommended SLURM variables

| Variables | Values     |
| :-------- | :--------- |
| Time      | 20 minutes |
| Memory    | 4GB        |
| CPUs      | 16         |


### Concatenate output fasta files

The relevant outputs from **CheckV** are `viruses.fna` and `proviruses.fna`. This step also modifies contig headers for readability in cases where checkV has trimmed any residual host sequence off the end of integrated prophage sequence (this is separate and additional to previous prophage excision by **VIBRANT** or **VirSorter2**).

```bash
cat viral_identification/perSample_checkv/viruses.fna viral_identification/perSample_checkv/proviruses.fna > viral_identification/perSample_checkv/viral_contigs.fna 

sed -i -e "s/\s/__excised_start_/g" -e "s/-/_end_/g" -e "s/\//_len_/g" -e "s/|/_/" -e "s/|//g" viral_identification/perSample_checkv/viral_contigs.fna
```

### Add CheckV results to summary table

> **Note**<br>
> This may ultimately be put into a script for ease of use. But for now we can use the python code below.

```bash
cd viral_identification

# Load modules
module purge
module load Python/3.8.2-gimkl-2020a

# Call Python
python3
```

```python
import pandas as pd
import numpy as np

# Import summary table (manually set dtypes)
summary_table = pd.read_csv('summary_tables/viral_contigs.summary_table_VIRUSES.txt', sep='\t', \
                            dtype={'contig_ID': str, \
                                   'vibrant_contig_ID': str, \
                                   'vibrant_total_genes': float, \
                                   'vibrant_KEGG_genes': float, \
                                   'vibrant_KEGG_v_score': float, \
                                   'vibrant_Pfam_genes': float, \
                                   'vibrant_Pfam_v_score': float, \
                                   'vibrant_VOG_genes': float, \
                                   'vibrant_VOG_v_score': float, \
                                   'vsort2_contig_ID': str, \
                                   'vsort2_max_score': float, \
                                   'vsort2_max_score_group': str, \
                                   'vsort2_hallmark_genes': float, \
                                   'vsort2_viral_component': float, \
                                   'vsort2_cellular_component': float, \
                                   'dvfind_contig_ID': str, \
                                   'dvfind_score': float, \
                                   'dvfind_p_adj': float})

# Import CheckV table
checkv = pd.read_csv('perSample_checkv/quality_summary.tsv', sep='\t', \
                     dtype={'contig_id': str, \
                            'contig_length': float, \
                            'provirus': str, \
                            'proviral_length': float, \
                            'gene_count': float, \
                            'viral_genes': float, \
                            'host_genes': float, \
                            'checkv_quality': str, \
                            'miuvig_quality': str, \
                            'completeness': float, \
                            'completeness_method': str, \
                            'contamination': float, \
                            'kmer_freq': float, \
                            'warnings': str})

# Add CheckV prefix and modify column name
checkv = checkv.add_prefix('checkv_')
checkv['contig_ID'] = checkv['checkv_contig_id']

# Merge tables and write data
summary_table = pd.merge(summary_table, checkv, left_on="contig_ID", right_on="contig_ID", how='outer')
summary_table.to_csv('2.summary_tables/viral_contigs.summary_table_VIRUSES_checkv.txt', sep='\t', index=False)

quit()
```

### Filter putative 'viral' contigs by CheckV results

The script `checkv_filter_contigs.py` further filters the sets of viral contigs based on **CheckV** results. By default, this retains only contigs where: ((viral_genes > 0) OR (viral_genes = 0 AND host_genes = 0). This script takes the **CheckV** outputs (including the `proviruses.fna` and `viruses.fna` files, and `quality_summary.tsv`), and returns fasta and `quality_summary.tsv` files with 'filtered' appended to the file name.

```bash
# Load modules
module purge
module load Python/3.8.2-gimkl-2020a

checkv_filter_contigs.py \
  --checkv_dir_input viral_identification/perSample_checkv/
  --output_prefix viral_identification/perSample_checkv/viral_contigs
```

# Dereplication (across-assemblies)

Contigs identified so far now need to be dereplicated into one final set of representative (clustered) contigs which are referred to as 'viral operational taxonomic units' (vOTUs), each of which represent distinct viral 'populations'. Here, we can use the `Cluster_genomes_5.1.pl` developed by Simon Roux's group: https://github.com/simroux/ClusterGenomes

This script clusters contigs based on sequence similarity thresholds, returning a representative (vOTU) sequence for each cluster. [Roux et al. (2018)](https://doi.org/10.1038/nbt.4306) recommends a threshold of 95% similarity over 85% of the sequence length, based on currently available data.

In the case where multiple assemblies have been analysed, this step is necessary to reduce the viral data down to one representative set for all assemblies (which is important for read mapping to assess differntial coverage across assemblies, for example). If only one assembly data set has been processed, this step is still useful to reduce the data down into meaningful units for downstream analyses (i.e. viral 'populations' rather than unique sequences).

Note:

- Download the Cluster_genomes_5.1.pl script from https://github.com/simroux/ClusterGenomes
- mummer is also required. Download the latest version and add the path to the bin directory in the Cluster_genomes_5.1.pl script below.

### Download `Cluster_genomes_5.1.pl`

```bash
wget https://raw.githubusercontent.com/simroux/ClusterGenomes/master/Cluster_genomes_5.1.pl
chmod 777 Cluster_genomes_5.1.pl
```

### Install MUMmer (if necessary)

```bash
# Download and unpack
wget https://github.com/mummer4/mummer/releases/download/v4.0.0rc1/mummer-4.0.0rc1.tar.gz
tar -xzf mummer-4.0.0rc1.tar.gz

# Install
cd mummer-4.0.0rc1/
./configure --prefix=/path/to/Software/mummer_v4.0.0
make
make install
```

### File preparation for multiple assemblies

Assuming there are 9 assemblies, each of which represents a sample:

```bash
# Load modules
module purge
module load BBMap/38.95-gimkl-2020a 

# Output directories
mkdir -p viral_identification/cluster_vOTUs

# Concatenate files
for i in {1..9}; do
    cat viral_identification/perSample_checkv/sample_${i}/viral_contigs_filtered.fna >> viral_identification/cluster_vOTUs/viral_contigs_allSamples.fna
done

# Sort by sequence size
sortbyname.sh \
  in=viral_identification/cluster_vOTUs/viral_contigs_allSamples.fna \
  out=viral_identification/cluster_vOTUs/viral_contigs_allSamples.sorted.fna \
  length descending
```

### Run cluster_genomes.pl

Run cluster_genomes.pl at min identity = 95% similarity over at least 85% of the shortest contig

```bash
# Set up working directories
cd viral_identification/cluster_vOTUs

# Run
/path/to/scripts/Cluster_genomes_5.1.pl \
  -f viral_contigs_allSamples.sorted.fna \
  -d /path/to/Software/mummer_v4.0.0/bin/ \
  -t 20 \
  -c 85 \
  -i 95
```

<!-- 8 Jan 2023 -->

### Check total number of clustered contigs (vOTUs)

```bash
cd /working/dir

# count contigs in cluster output file
grep -c ">" 1.viral_identification/5.cluster_vOTUs/viral_contigs_allSamples.sorted_95-85.fna
```

### Optional: Modify derep contig headers to be vOTU_n

- It can be useful for downstream processing to standardise the contig headers of the cluster representative sequences; for example, to replace all headers with vOTU_n.
- The script below replaces all headers with vOTU_n and create a table file of vOTU_n ids against the original full contig headers (of the representative sequences from each cluster).
  - Note: Cluster_genomes_5.1.pl also outputs a file matching cluster representative sequences to each of the sequences that are contained in the cluster.
- Optional: you may also wish to omit this step here and instead run it after having calculated differential coverage across sample assmeblies (via read mapping), to enable first ordering the contigs by abundance (coverage), and then generating vOTU_n headers
- This may ultimately be put into a script for ease of use. But for now we can use the python code below

```bash
cd /working/dir/1.viral_identification/5.cluster_vOTUs

# LOAD PYTHON
module purge
module load Python/3.8.2-gimkl-2020a
python3
import os
import pandas as pd
import numpy as np
import re
from Bio.SeqIO.FastaIO import SimpleFastaParser

fasta_in = 'viral_contigs_allSamples.sorted_95-85.fna'
fasta_out = 'vOTUs.fna'
lookup_table_out = 'vOTUs_lookupTable.txt'

# Read in fasta file, looping through each contig
# rename contig headers with incrementing vOTU_n headers
# write out new vOTUs.fna file and tab-delimited table file of matching vOTU_n and contigID headers.
i=1
with open(fasta_in, 'r') as read_fasta:
    with open(fasta_out, 'w') as write_fasta:
        with open (lookup_table_out, 'w') as write_table:
            write_table.write("vOTU" + "\t" + "cluster_rep_contigID" + "\n")
            for name, seq in SimpleFastaParser(read_fasta):
                write_table.write("vOTU_" + str(i) + "\t" + name + "\n")
                write_fasta.write(">" + "vOTU_" + str(i) + "\n" + str(seq) + "\n")
                i += 1

quit()
```

# Assessing vOTUs and additional filtering

### CheckV on vOTUs

Re-run CheckV, this time on the dereplicated contig set (vOTUs) to output checkV stats

```bash
#!/bin/bash -e
#SBATCH -A your_project_account
#SBATCH -J 3_checkv_vOTUs
#SBATCH --time 01:00:00
#SBATCH --mem=4GB
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=16
#SBATCH -e 3_checkv_vOTUs.err
#SBATCH -o 3_checkv_vOTUs.out
#SBATCH --profile=task

# Load dependencies
module purge
module load CheckV/0.7.0-gimkl-2020a-Python-3.8.2

# Set up working directories
cd /working/dir/1.viral_identification
mkdir -p 6.checkv_vOTUs

# Run main analyses 
checkv_in="5.cluster_vOTUs/vOTUs.fna"
checkv_out="6.checkv_vOTUs"
srun checkv end_to_end ${checkv_in} ${checkv_out} -t 16 --quiet
```

### CheckV: Concatenate output fasta files (viruses.fna and proviruses.fna)

```bash
cd /working/dir/1.viral_identification

# concatenate viruses and prophage files
cat 6.checkv_vOTUs/viruses.fna  6.checkv_vOTUs/proviruses.fna >  6.checkv_vOTUs/vOTUs.checkv.fna 
# modify checkv prophage contig headers
sed -i -e "s/\s/__excised_start_/g" -e "s/-/_end_/g" -e "s/\//_len_/g" -e "s/|/_/" -e "s/|//g" 6.checkv_vOTUs/vOTUs.checkv.fna
```

### Filter vOTUs based on checkV results

NOTE:
- As we have run checkv_filter_contigs.py on all individual contigs (in the per-sample QC step previously), this may be redundant here?
- In which case, simply proceed with the concatenated file above, rather than the ..._filtered... files output by checkv_filter_contigs.py

```bash
# Load dependencies
module purge
module load Python/3.8.2-gimkl-2020a
export PATH="/nesi/project/uoa02469/custom-scripts/MikeH/:$PATH"

# Set up working directories
cd /working/dir/1.viral_identification

# Run for vOTUs
/path/to/scripts/checkv_filter_contigs.py \
    --checkv_dir_input 6.checkv_vOTUs/ \
    --output_prefix 6.checkv_vOTUs/vOTUs.checkv
```

<br>

# Final set of dereplicated viral contigs

At this stage we have a final set of dereplicated viral contigs for all downstream analyses.

Key files include:

- Final dereplicated viral contig data set: /working/dir/1.viral_identification/6.checkv_vOTUs/vOTUs.checkv_filtered.fna
  - (Or vOTUs.checkv.fna, if the final checkv_filter_contigs.py step was not run)
- checkV stats for dereplicated viral contigs: /working/dir/1.viral_identification/6.checkv_vOTUs/vOTUs.checkv_filtered_quality_summary.txt

<br>

# Additional Resources

### Notes on manual curation of vOTUs and general resources

Some very helpful notes on manual curation are available at the VirSorter2 protocols page here: https://doi.org/10.17504/protocols.io.bwm5pc86

Further valuable reading on Minimum Information about an Uncultivated Virus Genome (MIUViG): https://doi.org/10.1038/nbt.4306

A great resource on standards in viromics: https://doi.org/10.7717/peerj.11447