# Pre-processing reads

This is the basic process of ensuring the quality of raw sequence reads, and removing (or trimming) reads which fail a pre-defined quality threshold either across the whole read, or in some sub-section of the read. The commands below are written assuming Illumina paired-end sequencing (MiSeq, HiSeq, NextSeq etc.).

There are three good quality trimming tools available and it is the users choice which to use.

1. [**sickle**](https://github.com/najoshi/sickle) v1.33
1. [**trimmomatic**](http://www.usadellab.org/cms/index.php?page=trimmomatic) v0.39
1. [**BBMap**](https://jgi.doe.gov/data-and-tools/bbtools/bb-tools-user-guide/bbmap-guide/) v38.95

Only `trimmomatic` and `BBMap` are on NeSI as modules. You can obtain `sickle` from the link provided. It is lightweight and open-source.

The examples below are given with approximately equivalent settings but there are a few differences in behaviour that cannot be captured. Specifically, these commands will run with the following specifications:

* FastQ files are in phred33 encoding
* We require a minimum quality score of Q30
* We will only accept sequences longer than 80 bp after trimming
* Orphan sequences (sequences which pass filtering, but their paired partner does not) are to be reported

These should be changed according to your sequencing platform and read length obtained.

## Prepare files

If you have samples that were sequenced over multiple lanes, concatenate the files for each read (R1 or R2, **separately**) into sets of paired read files per sample. It is also advisable to rename sample files to a simpler format for ease of downstream processing (e.g. sampleA_R1, sampleA_R2, sampleB_R1, sampleB_R2...)

## sickle

```bash
sickle pe -t sanger -n -q 30 -l 80 \
          -f SampleA_R1.fq.gz -r SampleA_R2.fq.gz \
          -o SampleA_R1.trim.fq -p SampleA_R2.trim.fq \
          -s SampleA_single.trim.fq
```

The only flag in this example that differs from the list above is the **-n** flag, which tells sickle to cut the sequence at the first *N* base. Unlike the next two examples, **sickle** does not allow for multithreaded trimming.

## Trimmomatic

```bash
trimmomatic PE -threads 10 -phred33 LEADING:3 TRAILING:3 \
               SLIDINGWINDOW:4:30 MINLEN:80 \
               SampleA_R1.fq.gz SampleA_R2.fq.gz \
               SampleA_R1.trim.fq.gz SampleA_R1.trim_single.fq.gz \
               SampleA_R2.trim.fq.gz SampleA_R2.trim_single.fq.gz

cat SampleA_R1.trim_single.fq.gz SampleA_R2.trim_single.fq.gz > SampleA_single.trim.fq.gz
```

Because **trimmomatic** exports separate orphan files for the forward and reverse reads, I just merge them together using **cat**. Some of the parameters for trimmomatic are a bit cryptic, so briefly:

1. *SLIDINGWINDOW:4:30*: This tells **trimmomatic** to read the sequence 4 base pairs at a time, and truncate the read if the average quality score over this window is less
than 30.
1. *LEADING:3*: Cut the start of the sequence so that there are no leading *N* bases or bases with Q<3.
1. *TRAILING:3*: - Cut the end of the sequence so that there are no tailing *N* bases or bases with Q<3.

## BBMap

BBMap is a program released by the JGI which does just about everything. To run its quailty filtering protocol with similar settings to above:

```bash
bbduk.sh qtrim=rl trimq=30 \
         in1=SampleA_R1.fq.gz in2=SampleA_R2.fq.gz \
         out1=SampleA_R1.trim.fq out2=SampleA_R2.trim.fq outs=SampleA_single.trim.fq.gz
```

However, it is also possible to simultaneously screen your reads for contaminant PhiX sequences while performing the quality checking. To modify the above command:

```bash
bbduk.sh qtrim=rl trimq=30 \
         stats=Sample.phix.txt k=31 hdist=1 ref=$BBMAP_phix \
         in1=SampleA_R1.fq.gz in2=SampleA_R2.fq.gz \
         out1=SampleA_R1.trim.fq out2=SampleA_R2.trim.fq outs=SampleA_single.trim.fq.gz \
         outm1=SampleA_R1.phix.fq outm2=SampleA_R2.phix.fq outms=SampleA_s.phix.fq.gz
```

This will use rapid k-mer profiling to identify reads that look like PhiX genome data, and split them off into the 'phix' output files. Please note that the variable **$BBMAP_phix** is defined when you run the **genome_pipe** command. If you are loading **BBMap** from other means, you will need to download the appropriate reference yourself. The PhiX reference is included in the [package](https://sourceforge.net/projects/bbmap/) found in the *reference* directory.

<br>

# Assembly

The first step in obtaining metagenome-assembled genomes (MAGs) is to assemble the metagenomic reads into a combined assembly. There are three assemblers provided under genome pipe, each of which has differing strengths and weaknesses. As a general rule, **SPAdes** is the best metagenomic assembler currently available, but it comes with a steep memory cost and comparatively long run times. Two good alternatives are **IDBA-UD** and **MegaHIT**, which typically offer slightly inferior assemblies but are significantly faster and cheaper to run.

Do note, however, that genome assembly is a highly variable process and there may be samples which assemble better under any one of the assemblers. Most literature on the subject places **SPAdes** as the top assembler on average, but this is not a guarantee for your data set. It is also worth noting that **IDBA-UD** or **MegaHIT** are more than likely to provide you with sufficient data to address your hypothesis. [Vollmers et al. (2017)](https://doi.org/10.1371/journal.pone.0169662) provides some guidance for choosing assemblers:
> "If micro diversity is not a major issue, and the primary research goal is to bin and reconstruct representative bacterial genomes from a given environment, metaSPAdes should clearly be the assembler of choice... If micro diversity is however an issue, or if the degree of captured diversity is far more important than contig lengths, then IDBA-UD or MegaHIT should be preferred."

1. [**SPAdes**](https://github.com/ablab/spades) v3.15.3
1. [**IDBA**](https://github.com/loneknightpy/idba) v1.1.3
1. [**MegaHIT**](https://github.com/voutcn/megahit) v1.1.4

The above softwares are all available on NeSI as modules. Please note that IDBA and IDBA-UD are separate modules on NeSI. 

> **Single or co-assemblies?** <br>
> Depending on the study question, and available time and computational resources, you may wish to do single assemblies (i.e. separate assemblies per sample) or some variety of co-assemblies. Examples of approaches to co-assemblies:
> - Full co-assembly (i.e. all samples assembled together)
> - Co-assemblies based on sampling design/strategy (e.g. separate co-assemblies of sediment and water column samples, co-assemblies based on discrete time points)
> 
> Some benefits of co-assemblies: 
> - Higher read-depth
> - Potentially better at recovering rare taxa that occur in more than 1 sample
> - One reference assembly for read mapping (if working from a full co-assembly) 
> 
> [Vosloo et al. (2021)](https://doi.org/10.1128/Spectrum.01434-21) compares different assembly and binning strategies and may provide some insight to help you decide on the best course of action.

> **Note** <br>
> The following workflow is geared towards co-assemblies. If running single assemblies, simply omit concatenation by sample.

## SPAdes

When performing metagenomic co-assembly, it is desirable to maintain the individuality of each sample during assembly, so that the assembler can account for coverage variation in a sample-specific manner. While this functionality is technically in **SPAdes**, in practice it never seems to work for metagenomic assemblies and it is neccessary to concatenate your files together.

To run **SPAdes** in its most basic mode, with 16 threads use the command

```bash
cat SampleA_R1.trim.fq SampleB_R1.trim.fq > Samples_R1.trim.fq
cat SampleA_R2.trim.fq SampleB_R2.trim.fq > Samples_R2.trim.fq

spades.py --meta -t 16 -1 Samples_R1.trim.fq -2 Samples_R2.trim.fq -o Sample_spades/
```

However, there are a number of things you may need to tweak for an optimal assembly. For starters, **SPAdes** imposes a 250 GB memory limit on itself while running, which is almost never enough for metagenomic assembly. The cap can be raised with the **-m** flag. Also worth consiering is the k-mer size range for performing assembly. By default, **SPAdes** starts with *k=21*, and increases in steps of 22 (k=21, k=33, k=55 etc) until it determines that it is not worth proceeding. This can be overwritten with the **-k** flag to specify the k values to assemble with.

Finally, owing to its internal read correction protocol, **SPAdes** produces a number of large temporary files while running. This shouldn't matter if you are working in the *nobackup/* directory on NeSI, but if you are concerned about running out of project space, you can redirect these temporary files to another location using the **–tmp-dir** flag.

To run **SPAdes** with all these parameters above configured:

```bash
spades.py --meta -m 900 -t 16 -k 41,61,81,101,127 -1 Samples_R1.trim.fq -2 Samples_R2.trim.fq -o Sample_spades/
```

## IDBA-UD

Unlike almost every other assembler, **IDBA-UD** requires interleaved fasta files as the input, instead of paired fastq files. Fortunately, it comes bundled with some tools to convert your data into this format. It also accepts only a single input file, so a concatenation step is required just like for **SPAdes**.

It’s also important to keep an eye on the length of your reads, because **IDBA-UD** uses different flags for short (**-r**) and long (**-l**) sequences. These are determined when **IDBA-UD** is compiled, and depending on your sequencing and quailty filtering your data could fall into either of these classes.

```bash
fq2fa --merge --filter SampleA_R1.trim.fq SampleA_R2.trim.fq SampleA.fna
fq2fa --merge --filter SampleB_R1.trim.fq SampleB_R2.trim.fq SampleB.fna
cat SampleA.fna SampleB.fna > Samples.fna

idba_ud --seed_kmer 21 --step 22 -l Samples.fna -o Sample_idba/
```

## MegaHIT

The only novel feature worth pointing out with a basic **MegaHIT** assembly is that while it has a built-in memory limit like **SPAdes**, you can specify this value as a percentage of your computers total memory rather than calculating an exact value. If this is a feature you want to use, it is set under the **-m** flag - if you pass a whole number then **MegaHIT** will treat it as a memory limit in GB. If you pass a fraction, it is treated as a percentage of total memory. The memory usage of **MegaHIT** is significantly lower than that of **SPAdes**, so if you are setting a cap it does not need to be as high as for an equivalent **SPAdes** job. Also like **SPAdes**, **MegaHIT** checkpoints its runs, so if a job fails or times out on the server, you can restart the assembly by repeating the same command, with the **–continue** flag.

```bash
megahit --k-list 41,61,81,101,127 -m 400 -t 16 \
        -1 SampleA_R1.trim.fq,SampleB_R1.trim.fq \
        -2 SampleA_R2.trim.fq,SampleB_R2.trim.fq \
        -o Sample_megahit/
```

<br>

# Evaluation
The following are examples of how you can assess your assembly. This is also useful if you have tested different assembly parameters (e.g. different k-mer sizes) or different assemblers on a subset of samples and are deciding what parameter settings to use for the proper assemblies.

Here we will be looking at the following:

- counts of contigs output by each assembly (including the filtered vs unfiltered assemblies)
- relative length of contigs output by each assembly via contig N/L50 values (an indication of the relative length of contigs in each of the assemblies)

You can use these metrics (among others) to select the assembly parameters (and/or assembler) you wish to proceed with for the actual assemblies of all samples (or co-assembly, or multiple mini co-assemblies, if that is the option you go for).


> **Note** <br> 
> More contigs may not necessarily mean better assembly. An assembly with fewer contigs but with contigs of greater length on average may be preferred. Ultimately, this is a bit of a trial-and-error process, and what entails the "best" assembly may depend on both your data and the question you're asking downstream.

## Counting contigs
This is a quick and dirty way to find out how many contigs/scaffold seqeunces were assembled.

```bash
grep -c '>' assembly/scaffolds.fasta
```

## Basic assembly statistics

The wrapper script `stats.sh` from BBMap is a helpful tool to glance at information such as:
- Total number of contigs/scaffolds
- Summary statistics[^1]: N/L50, N/L90
- Length distributions

```bash
module load BBMap/38.73-gimkl-2018b

stats.sh in=scaffolds.fasta
```

## Reference-informed assembly evaluation
<!--
MetaQUAST here
- with ref
- without ref
-->

[^1]: The N50 and L50 outputs from the above script is reversed. For example: <br>
`Main genome scaffold N/L50:  82/27.296 KB` <br>
The above should be interpreted as N50 = 27.296 KB and L50 = 82.